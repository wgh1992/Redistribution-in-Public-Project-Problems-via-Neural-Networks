{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:04.225695Z",
     "start_time": "2021-06-13T14:32:02.593334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pygame\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as opt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats as st\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from matplotlib.colors import LogNorm \n",
    "import matplotlib.cm as cm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy.interpolate import griddata\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "\n",
    "print(dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:04.241287Z",
     "start_time": "2021-06-13T14:32:04.226693Z"
    }
   },
   "outputs": [],
   "source": [
    "global temp_list\n",
    "temp_list = []\n",
    "Agent_number_n=10;\n",
    "Alpha = 0.88 \n",
    "\n",
    "# Hyper Parameters\n",
    "echo = 2001 \n",
    "BATCH_SIZE = 64\n",
    "LR_G = 0.001           # learning rate for generator\n",
    "LR_D = 0.001           # learning rate for discriminator\n",
    "N_IDEAS = Agent_number_n             # think of this as number of ideas for generating an art work (Generator)\n",
    "ART_COMPONENTS = Agent_number_n     # it could be total point G can draw in the canvas\n",
    "\n",
    "Is_GAN = True # if use Gan\n",
    "\n",
    "def Generate_distribution(Agent_number_n):\n",
    "    return sorted(np.random.rand(Agent_number_n), reverse=True)\n",
    "    #return sorted(np.random.normal(normalloc,normalscale,Agent_number_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:04.838422Z",
     "start_time": "2021-06-13T14:32:04.243283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.73852936 0.67280578 0.63423009 ... 0.31878063 0.25127298 0.19922378]\n",
      "  [0.88658398 0.67280578 0.63423009 ... 0.31878063 0.25127298 0.19922378]\n",
      "  [0.88658398 0.73852936 0.63423009 ... 0.31878063 0.25127298 0.19922378]\n",
      "  ...\n",
      "  [0.88658398 0.73852936 0.67280578 ... 0.35899018 0.25127298 0.19922378]\n",
      "  [0.88658398 0.73852936 0.67280578 ... 0.35899018 0.31878063 0.19922378]\n",
      "  [0.88658398 0.73852936 0.67280578 ... 0.35899018 0.31878063 0.25127298]]\n",
      "\n",
      " [[0.79049687 0.71101666 0.69311714 ... 0.34053834 0.30079014 0.29302064]\n",
      "  [0.81223842 0.71101666 0.69311714 ... 0.34053834 0.30079014 0.29302064]\n",
      "  [0.81223842 0.79049687 0.69311714 ... 0.34053834 0.30079014 0.29302064]\n",
      "  ...\n",
      "  [0.81223842 0.79049687 0.71101666 ... 0.366808   0.30079014 0.29302064]\n",
      "  [0.81223842 0.79049687 0.71101666 ... 0.366808   0.34053834 0.29302064]\n",
      "  [0.81223842 0.79049687 0.71101666 ... 0.366808   0.34053834 0.30079014]]\n",
      "\n",
      " [[0.77177326 0.61326579 0.56754411 ... 0.31400808 0.1638248  0.09753289]\n",
      "  [0.86023373 0.61326579 0.56754411 ... 0.31400808 0.1638248  0.09753289]\n",
      "  [0.86023373 0.77177326 0.56754411 ... 0.31400808 0.1638248  0.09753289]\n",
      "  ...\n",
      "  [0.86023373 0.77177326 0.61326579 ... 0.33640279 0.1638248  0.09753289]\n",
      "  [0.86023373 0.77177326 0.61326579 ... 0.33640279 0.31400808 0.09753289]\n",
      "  [0.86023373 0.77177326 0.61326579 ... 0.33640279 0.31400808 0.1638248 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.47674413 0.26219942 0.21406134 ... 0.10117896 0.07437221 0.07410519]\n",
      "  [0.82644463 0.26219942 0.21406134 ... 0.10117896 0.07437221 0.07410519]\n",
      "  [0.82644463 0.47674413 0.21406134 ... 0.10117896 0.07437221 0.07410519]\n",
      "  ...\n",
      "  [0.82644463 0.47674413 0.26219942 ... 0.11167614 0.07437221 0.07410519]\n",
      "  [0.82644463 0.47674413 0.26219942 ... 0.11167614 0.10117896 0.07410519]\n",
      "  [0.82644463 0.47674413 0.26219942 ... 0.11167614 0.10117896 0.07437221]]\n",
      "\n",
      " [[0.84627645 0.83026222 0.73997065 ... 0.42943787 0.15700782 0.07072046]\n",
      "  [0.95050932 0.83026222 0.73997065 ... 0.42943787 0.15700782 0.07072046]\n",
      "  [0.95050932 0.84627645 0.73997065 ... 0.42943787 0.15700782 0.07072046]\n",
      "  ...\n",
      "  [0.95050932 0.84627645 0.83026222 ... 0.44494035 0.15700782 0.07072046]\n",
      "  [0.95050932 0.84627645 0.83026222 ... 0.44494035 0.42943787 0.07072046]\n",
      "  [0.95050932 0.84627645 0.83026222 ... 0.44494035 0.42943787 0.15700782]]\n",
      "\n",
      " [[0.92442365 0.80035357 0.71869687 ... 0.42762522 0.03856249 0.01669747]\n",
      "  [0.99166885 0.80035357 0.71869687 ... 0.42762522 0.03856249 0.01669747]\n",
      "  [0.99166885 0.92442365 0.71869687 ... 0.42762522 0.03856249 0.01669747]\n",
      "  ...\n",
      "  [0.99166885 0.92442365 0.80035357 ... 0.6435403  0.03856249 0.01669747]\n",
      "  [0.99166885 0.92442365 0.80035357 ... 0.6435403  0.42762522 0.01669747]\n",
      "  [0.99166885 0.92442365 0.80035357 ... 0.6435403  0.42762522 0.03856249]]]\n",
      "[5.10716048 5.56383598 4.48927012 ... 2.53248107 5.73826796 5.90423354]\n"
     ]
    }
   ],
   "source": [
    "def h_3_star(a, b, t):\n",
    "    return a - min(a, t) + b - min(b, t) + max(min(a, t)+min(b, t), 2*t/3) + 1/2 * max(min(a, t)+min(b, t), t) - 1/2 * max(max(min(a, t), min(b, t)), 2*t/3) - t/6\n",
    "\n",
    "\n",
    "def f_function(a, b, z):\n",
    "    if(z >= 1):\n",
    "        return (a+b)/2 + z/3\n",
    "    else:\n",
    "        return z/3 + h_3_star(a, b, 1-z)/2\n",
    "\n",
    "def h_function_label(input_list):\n",
    "    #input_list = sorted(input_list)\n",
    "    g_list = []\n",
    "    for j1 in range(len(input_list) ):\n",
    "        for j2 in range(len(input_list)):\n",
    "            if(j1 != j2):\n",
    "                a = input_list[j1]\n",
    "                b = input_list[j2]\n",
    "                z = sum(input_list)- a-b\n",
    "\n",
    "                g_list.append( f_function(a, b, z) * (Agent_number_n-1))\n",
    "    h = sum(g_list) * 3 /  (Agent_number_n) /  (Agent_number_n-1) /  (Agent_number_n - 2)\n",
    "    return h\n",
    "\n",
    "def appen(_x_list,y):\n",
    "    global temp_list\n",
    "    temp_list.append(_x_list)\n",
    "    \n",
    "def appen_train(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "        \n",
    "        h = h_function_label(temp)\n",
    "        h_list.append(float(h))\n",
    "    temp_list = np.array(temp_list)\n",
    "    x_list = np.array(x_list)\n",
    "    return temp_list,S,x_list,h_list\n",
    "    \n",
    "\n",
    "def appen_test(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    testing_data.append(temp_list)\n",
    "    testing_label.append(S)\n",
    "    temp_list = np.array(temp_list)\n",
    "    return temp_list,S\n",
    "    \n",
    "\n",
    "def read_testing_data():\n",
    "    for i in range(10000):\n",
    "        appen_test(Generate_distribution(Agent_number_n));\n",
    "                            \n",
    "\n",
    "testing_data=[]\n",
    "testing_label=[]\n",
    "S=1.0\n",
    "read_testing_data();\n",
    "\n",
    "testing_data=np.array(testing_data)\n",
    "testing_label=np.array(testing_label)\n",
    "print(testing_data)\n",
    "print(testing_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:04.853383Z",
     "start_time": "2021-06-13T14:32:04.839420Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.input_dim = (Agent_number_n-1)\n",
    "        self.hidden_dim = 128\n",
    "        self.output_dim = 1\n",
    "        self.hidden_layer_count = 6 \n",
    "        \n",
    "        current_dim = self.input_dim\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(self.hidden_layer_count):\n",
    "            self.layers.append(torch.nn.Linear(current_dim, self.hidden_dim))\n",
    "            current_dim = self.hidden_dim\n",
    "        self.layers.append(torch.nn.Linear(current_dim, self.output_dim))\n",
    "\n",
    "    def calculate(self, value_list):\n",
    "        h = value_list\n",
    "        LeakyReLU = torch.nn.LeakyReLU()\n",
    "        for layer in self.layers:\n",
    "            h = torch.relu(layer(h))\n",
    "        return h\n",
    "\n",
    "    def forward(self, input_list,input_label,list_x):\n",
    "        global iteration,echo,target_order\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        input_list = torch.from_numpy(\n",
    "            np.array(input_list)).to(dev).type(torch.float32)\n",
    "        h_list = []\n",
    "\n",
    "        for i in range(Agent_number_n):\n",
    "            h = self.calculate(input_list[i])\n",
    "            h_list.append(h)\n",
    "#             loss3 += torch.square(h_function_2(input_list)-h2[1])\n",
    "            \n",
    "        input_label = torch.from_numpy(\n",
    "            np.array(input_label)).to(dev).type(torch.float32)\n",
    "        sum_h = torch.sum(torch.cat(h_list)).to(dev)\n",
    "\n",
    "\n",
    "        loss1 = torch.where((Agent_number_n-1)*input_label>sum_h,\n",
    "                        torch.square(((Agent_number_n-1)*input_label-sum_h)),\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "        loss2 = torch.where((Agent_number_n-Alpha)*input_label<sum_h,\n",
    "                        torch.square((sum_h-(Agent_number_n-Alpha)*input_label))/100,\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "\n",
    "        return loss1,loss2\n",
    "    \n",
    "    def supervised_loss(self, input_list,label):\n",
    "        global iteration,echo,target_order\n",
    "        input_list = torch.from_numpy(\n",
    "            np.array(input_list)).to(dev).type(torch.float32)\n",
    "        loss = 0 \n",
    "        for i in range(Agent_number_n):\n",
    "            h = self.calculate(input_list[i])\n",
    "            loss += torch.square(h - label[i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:04.883386Z",
     "start_time": "2021-06-13T14:32:04.854381Z"
    }
   },
   "outputs": [],
   "source": [
    "def redistribution_value_function(input_tensor):\n",
    "    S = torch.max(torch.sum(input_tensor), torch.ones(1).to(dev))\n",
    "    temp_list = []\n",
    "\n",
    "\n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        \n",
    "        for j in range(Agent_number_n):\n",
    "            if(i != j):\n",
    "                temp .append(input_tensor[j])\n",
    "                \n",
    "        temp = torch.stack(temp)\n",
    "        temp_list.append(temp)\n",
    "    return torch.stack(temp_list), S\n",
    "\n",
    "GeneratorNet = nn.Sequential(                      # Generator\n",
    "    # random ideas (could from normal distribution)\n",
    "    nn.Linear(N_IDEAS, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    # making a painting from these random ideas\n",
    "    nn.Linear(64, ART_COMPONENTS),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.060117Z",
     "start_time": "2021-06-13T14:32:04.884345Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "random.seed(2000)\n",
    "torch.manual_seed(256)\n",
    "DiscriminatorNet  = Net()\n",
    "DiscriminatorNet.apply(weight_init)\n",
    "GeneratorNet.apply(weight_init)\n",
    "# DiscriminatorNet = torch.load(\"save/Deep_learning_D_10_1\")\n",
    "# GeneratorNet = torch.load(\"save/Deep_learning_G_10_1\")\n",
    "DiscriminatorNet.to(dev)\n",
    "GeneratorNet.to(dev)\n",
    "\n",
    "opt_D = torch.optim.Adam(DiscriminatorNet.parameters(), lr=LR_D)\n",
    "opt_G = torch.optim.Adam(GeneratorNet.parameters(), lr=LR_G)\n",
    "\n",
    "\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(opt_D, step_size=100, gamma=0.98)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(opt_G, step_size=100, gamma=0.98)\n",
    "\n",
    "index_train_list = []\n",
    "index_test_list = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.721088Z",
     "start_time": "2021-06-13T14:32:06.062112Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "supervised_loss() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0cdad87c013a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         training_data_i, training_S, training_data,label = appen_train(\n\u001b[0;32m     13\u001b[0m             Generate_distribution(Agent_number_n))\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mh_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminatorNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupervised_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mh_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: supervised_loss() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#supervised\n",
    "index_supervisedtrain_list = []\n",
    "supervisedtrain_losses = []\n",
    "for iteration in range(int(echo)):\n",
    "\n",
    "    temp_number = 0\n",
    "    total_batch_loss = 0 \n",
    "    \n",
    "    loss_sum = 0\n",
    "    denominator = 0\n",
    "    for index in range(0, BATCH_SIZE):\n",
    "        training_data_i, training_S, training_data,label = appen_train(\n",
    "            Generate_distribution(Agent_number_n))\n",
    "        h_loss = DiscriminatorNet.supervised_loss(training_data_i, label)\n",
    "        denominator += 1\n",
    "        loss_sum += h_loss\n",
    "\n",
    "    loss_sum = torch.sum(loss_sum)\n",
    "    loss = (loss_sum) / denominator \n",
    "    total_batch_loss += float(loss_sum)\n",
    "\n",
    "    opt_D.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_D.step()\n",
    "\n",
    "    temp_number = iteration\n",
    "    index_train_list.append(iteration)\n",
    "    train_losses.append(total_batch_loss)\n",
    "    scheduler_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.728970Z",
     "start_time": "2021-06-13T14:32:02.550Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylim(0, 1.0)\n",
    "plt.plot(index_supervisedtrain_list ,supervisedtrain_losses)\n",
    "plt.ylabel('supervised train loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.729969Z",
     "start_time": "2021-06-13T14:32:02.552Z"
    }
   },
   "outputs": [],
   "source": [
    "for iteration in range(int(echo)):\n",
    "\n",
    "    temp_number = 0\n",
    "    total_batch_loss = 0 \n",
    "        \n",
    "    loss2_list = []\n",
    "    loss1_sum = 0\n",
    "    loss2_sum = 0\n",
    "    denominator = 0\n",
    "    for index in range(0, BATCH_SIZE):\n",
    "        training_data_i, training_label, training_data, label = appen_train(\n",
    "            Generate_distribution(Agent_number_n))\n",
    "        h_loss1, h_loss2 = DiscriminatorNet(training_data_i, training_label,\n",
    "                                       training_data)\n",
    "        denominator += 1\n",
    "        loss1_sum += h_loss1\n",
    "        loss2_sum += h_loss2\n",
    "\n",
    "    loss_sum = torch.sum(loss1_sum + loss2_sum)\n",
    "    loss = (loss_sum) / denominator \n",
    "    total_batch_loss +=float(loss_sum)\n",
    "\n",
    "    opt_D.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_D.step()\n",
    "\n",
    "    temp_number = iteration\n",
    "    index_train_list.append(iteration)\n",
    "    train_losses.append(total_batch_loss)\n",
    "\n",
    "    if (iteration%100 == 0):\n",
    "        print(temp_number,loss,float(loss1_sum),float(loss2_sum))\n",
    "    ## Gan \n",
    "        if(Is_GAN):## Gan Work traning GeneratorNet\n",
    "\n",
    "            DiscriminatorNet.requires_grad = True\n",
    "            GeneratorNet.requires_grad = True\n",
    "\n",
    "\n",
    "            for step in range(100):\n",
    "                # real painting from artist\n",
    "                G_ideas = torch.randn(BATCH_SIZE, N_IDEAS,\n",
    "                                      requires_grad=True).to(dev)  # random ideas\\n\n",
    "                # fake painting from G (random ideas)\n",
    "\n",
    "                G_values = GeneratorNet(G_ideas)\n",
    "                G_values , indices = torch.sort(G_values, descending=True)\n",
    "            #     print(artist_paintings)\n",
    "            #     print(G_paintings)\n",
    "\n",
    "                result_list = []\n",
    "                for index in range(BATCH_SIZE):\n",
    "                    h_list = []\n",
    "                    value_list_tensor, S_tensor = redistribution_value_function(\n",
    "                        G_values[index])\n",
    "                    for i in range(Agent_number_n):\n",
    "                        h = DiscriminatorNet.calculate(\n",
    "                            value_list_tensor[i].cuda().type(torch.float32))\n",
    "                        h_list.append(h)\n",
    "                    h_list = torch.stack(h_list)\n",
    "                    result_list.append(torch.sum(h_list)/S_tensor.cuda())\n",
    "                result_list = torch.stack(result_list)\n",
    "\n",
    "                diff_loss = torch.max(result_list)-torch.min(result_list)\n",
    "                G_loss = torch.max(- diff_loss)\n",
    "\n",
    "                opt_G.zero_grad()\n",
    "                G_loss.backward()\n",
    "                opt_G.step()\n",
    "\n",
    "            # real painting from artist\n",
    "                G_ideas = torch.randn(BATCH_SIZE, N_IDEAS,\n",
    "                                      requires_grad=True).to(dev)  # random ideas\\n\n",
    "                # fake painting from G (random ideas)\n",
    "\n",
    "                G_values = GeneratorNet(G_ideas)\n",
    "                G_values , indices = torch.sort(G_values, descending=True)\n",
    "            #     print(artist_paintings)\n",
    "            #     print(G_paintings)\n",
    "\n",
    "                result_list = []\n",
    "                for index in range(BATCH_SIZE):\n",
    "                    h_list = []\n",
    "                    value_list_tensor, S_tensor = redistribution_value_function(\n",
    "                        G_values[index])\n",
    "                    for i in range(Agent_number_n):\n",
    "                        h = DiscriminatorNet.calculate(\n",
    "                            value_list_tensor[i].cuda().type(torch.float32))\n",
    "                        h_list.append(h)\n",
    "                    h_list = torch.stack(h_list)\n",
    "                    result_list.append(torch.sum(h_list)/S_tensor.cuda())\n",
    "                result_list = torch.stack(result_list)\n",
    "\n",
    "                diff_loss = torch.max(result_list)-torch.min(result_list)\n",
    "\n",
    "                D_loss = torch.where((Agent_number_n-1)>torch.min(result_list),\n",
    "                    torch.square(((Agent_number_n-1)-torch.min(result_list))),\n",
    "                    torch.zeros(1).to(dev)\n",
    "                  )   + torch.where((Agent_number_n-Alpha)<torch.max(result_list),\n",
    "                                torch.square((torch.max(result_list)-(Agent_number_n-Alpha)))/10000,\n",
    "                                torch.zeros(1).to(dev)\n",
    "                              )\n",
    "\n",
    "\n",
    "                opt_D.zero_grad()\n",
    "                D_loss.backward()\n",
    "                opt_D.step()\n",
    "                \n",
    "            print(\"Gan:\",G_loss,D_loss)\n",
    "            print()\n",
    "\n",
    "            \n",
    "        result_list = []\n",
    "        for index in range(len(testing_data)):\n",
    "            h_list = []\n",
    "            for i in range(Agent_number_n):\n",
    "                h = DiscriminatorNet.calculate(\n",
    "                    torch.tensor(testing_data[index][i]).to(dev).type(\n",
    "                        torch.float32))\n",
    "                h_list.append(float(h))\n",
    "            result_list.append(sum(h_list) / testing_label[index])\n",
    "        print(max(result_list), min(result_list),\n",
    "              max(result_list) - min(result_list),\n",
    "              (sum(result_list) / len(result_list) - min(result_list))+ Agent_number_n-1 - min(min(result_list),Agent_number_n-1) )\n",
    "\n",
    "        index_test_list.append(iteration)\n",
    "        test_losses.append(\n",
    "            (sum(result_list) / len(result_list) - min(result_list))+ Agent_number_n-1 - min(min(result_list),Agent_number_n-1) )\n",
    "        print()\n",
    "        index_test_list.append(iteration)\n",
    "        test_losses.append(\n",
    "            max(result_list)-min(result_list) )\n",
    "        \n",
    "  \n",
    "        print()\n",
    "\n",
    "\n",
    "    scheduler_D.step()\n",
    "    scheduler_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.731964Z",
     "start_time": "2021-06-13T14:32:02.555Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylim(0, 1.0)\n",
    "plt.plot(index_test_list,test_losses)\n",
    "plt.ylabel('test loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.732959Z",
     "start_time": "2021-06-13T14:32:02.557Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(DiscriminatorNet, \"save/Deep_learning_D_10_1\")\n",
    "if(Is_GAN):\n",
    "    torch.save(GeneratorNet, \"save/Deep_learning_G_10_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.732959Z",
     "start_time": "2021-06-13T14:32:02.559Z"
    }
   },
   "outputs": [],
   "source": [
    "def h_3_star(a, b, t):\n",
    "    return a - min(a, t) + b - min(b, t) + max(min(a, t)+min(b, t), 2*t/3) + 1/2 * max(min(a, t)+min(b, t), t) - 1/2 * max(max(min(a, t), min(b, t)), 2*t/3) - t/6\n",
    "\n",
    "\n",
    "def f_function(a, b, z):\n",
    "    if(z >= 1):\n",
    "        return (a+b)/2 + z/3\n",
    "    else:\n",
    "        return z/3 + h_3_star(a, b, 1-z)/2\n",
    "\n",
    "def h_function(input_list):\n",
    "    #input_list = sorted(input_list)\n",
    "    g_list = []\n",
    "    for j1 in range(len(input_list) ):\n",
    "        for j2 in range(len(input_list)):\n",
    "            if(j1 != j2):\n",
    "                a = input_list[j1]\n",
    "                b = input_list[j2]\n",
    "                z = sum(input_list)- a-b\n",
    "\n",
    "                g_list.append( f_function(a, b, z) * (Agent_number_n-1))\n",
    "    h = sum(g_list) * 3 /  (Agent_number_n) /  (Agent_number_n-1) /  (Agent_number_n - 2)\n",
    "    return h\n",
    "                \n",
    "                \n",
    "x_list = []\n",
    "y_list = []\n",
    "z_list = []\n",
    "result_list = []\n",
    "for index in range(len(testing_data)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        x_list.append(testing_data[index][i][0])\n",
    "        y_list.append(testing_data[index][i][1])\n",
    "        h = h_function(testing_data[index][i])\n",
    "        z_list.append(float(h))\n",
    "        h_list.append(float(h))\n",
    "    result_list.append(sum(h_list)/testing_label[index]) \n",
    "    \n",
    "    \n",
    "print(max(result_list), min(result_list), max(result_list)-min(result_list),sum(result_list)/len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.733957Z",
     "start_time": "2021-06-13T14:32:02.560Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list)/len(result_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.734954Z",
     "start_time": "2021-06-13T14:32:02.562Z"
    }
   },
   "outputs": [],
   "source": [
    "denominator = 0\n",
    "result_list = []\n",
    "for index in range(len(testing_data)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        h = DiscriminatorNet.calculate(torch.tensor(testing_data[index][i]).to(dev).type(torch.float32))\n",
    "        h_list.append(float(h))\n",
    "    \n",
    "    #print(sum(h_list),h_list,sum(h_list)/testing_label[index])\n",
    "    result_list.append(sum(h_list)/testing_label[index])\n",
    "    \n",
    "\n",
    "print(max(result_list),min(result_list),max(result_list)-min(result_list))\n",
    "print(sum(result_list)/len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.735952Z",
     "start_time": "2021-06-13T14:32:02.563Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list)/len(result_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.737947Z",
     "start_time": "2021-06-13T14:32:02.564Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate 10000 testing data on GeneratorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.738944Z",
     "start_time": "2021-06-13T14:32:02.565Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def appen_test_G(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    testing_data_G.append(temp_list)\n",
    "    testing_label_G.append(S)\n",
    "# fake painting from G (random ideas)\n",
    "\n",
    "def read_testing_data_G():\n",
    "    for i in range(10000):\n",
    "        #appen_test_G(sorted(np.random.rand(Agent_number_n), reverse=True));\n",
    "        G_ideas = torch.randn(N_IDEAS).to(dev)  # random ideas\\n\n",
    "        G_values = GeneratorNet(G_ideas)\n",
    "        G_values , indices = torch.sort(G_values, descending=True)\n",
    "        appen_test_G(G_values.detach().cpu().numpy()) \n",
    "        \n",
    "testing_data_G = []\n",
    "testing_label_G = []\n",
    "read_testing_data_G()\n",
    "testing_data_G=np.array(testing_data_G)\n",
    "testing_label_G=np.array(testing_label_G)\n",
    "print(testing_data_G)\n",
    "print(testing_label_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.739941Z",
     "start_time": "2021-06-13T14:32:02.566Z"
    }
   },
   "outputs": [],
   "source": [
    "result_list_G = []\n",
    "for index in range(len(testing_data_G)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        h = DiscriminatorNet.calculate(torch.tensor(testing_data_G[index][i]).to(dev).type(torch.float32))\n",
    "        h_list.append(float(h))\n",
    "    \n",
    "    #print(sum(h_list),h_list,sum(h_list)/testing_label[index])\n",
    "    result_list_G.append(sum(h_list)/testing_label_G[index])\n",
    "    \n",
    "\n",
    "print(max(result_list_G),min(result_list_G),max(result_list_G)-min(result_list_G))\n",
    "print(sum(result_list_G)/len(result_list_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.741936Z",
     "start_time": "2021-06-13T14:32:02.568Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list_G,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list_G)/len(result_list_G), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list_G.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list_G)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list_G)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T14:32:06.742933Z",
     "start_time": "2021-06-13T14:32:02.569Z"
    }
   },
   "outputs": [],
   "source": [
    "final_list = result_list + result_list_G\n",
    "\n",
    "\n",
    "plt.hist(final_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(final_list)/len(final_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
