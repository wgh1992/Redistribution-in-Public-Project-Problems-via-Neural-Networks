{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:13.866058Z",
     "start_time": "2021-06-27T04:56:05.944519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pygame\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as opt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats as st\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from matplotlib.colors import LogNorm \n",
    "import matplotlib.cm as cm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy.interpolate import griddata\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "\n",
    "print(dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:13.881018Z",
     "start_time": "2021-06-27T04:56:13.868053Z"
    }
   },
   "outputs": [],
   "source": [
    "global temp_list\n",
    "temp_list = []\n",
    "Agent_number_n=11;\n",
    "Alpha = 0.7\n",
    "\n",
    "# Hyper Parameters\n",
    "echo = 1001 \n",
    "BATCH_SIZE = 64\n",
    "LR_G = 0.001           # learning rate for generator\n",
    "LR_D = 0.001           # learning rate for discriminator\n",
    "N_IDEAS = Agent_number_n             # think of this as number of ideas for generating an art work (Generator)\n",
    "ART_COMPONENTS = Agent_number_n     # it could be total point G can draw in the canvas\n",
    "\n",
    "Is_GAN = True # if use Gan\n",
    "\n",
    "def Generate_distribution(Agent_number_n):\n",
    "    return sorted(np.random.rand(Agent_number_n), reverse=True)\n",
    "    #return sorted(np.random.normal(normalloc,normalscale,Agent_number_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:13.896013Z",
     "start_time": "2021-06-27T04:56:13.882015Z"
    }
   },
   "outputs": [],
   "source": [
    "new_input = 3\n",
    "def Dimensionality_reduction(data_sorted):\n",
    "    out_data = torch.ones(new_input).cuda()\n",
    "    out_data[0] = torch.max(data_sorted) \n",
    "    out_data[1] = torch.sum(data_sorted) \n",
    "    \n",
    "    temp_list = []\n",
    "    for i in range(len(data_sorted)-1):\n",
    "        temp_list.append(data_sorted[i] - data_sorted[i + 1])\n",
    "    \n",
    "    temp_tensor = torch.stack(temp_list)    \n",
    "    out_data[2] = torch.max(temp_tensor) \n",
    "    return out_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:13.987762Z",
     "start_time": "2021-06-27T04:56:13.897973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5.76746922e-01 5.26583780e-01 4.18801350e-01 ... 1.08061150e-01\n",
      "   5.86076788e-02 5.09550679e-02]\n",
      "  [9.03064774e-01 5.26583780e-01 4.18801350e-01 ... 1.08061150e-01\n",
      "   5.86076788e-02 5.09550679e-02]\n",
      "  [9.03064774e-01 5.76746922e-01 4.18801350e-01 ... 1.08061150e-01\n",
      "   5.86076788e-02 5.09550679e-02]\n",
      "  ...\n",
      "  [9.03064774e-01 5.76746922e-01 5.26583780e-01 ... 1.65540096e-01\n",
      "   5.86076788e-02 5.09550679e-02]\n",
      "  [9.03064774e-01 5.76746922e-01 5.26583780e-01 ... 1.65540096e-01\n",
      "   1.08061150e-01 5.09550679e-02]\n",
      "  [9.03064774e-01 5.76746922e-01 5.26583780e-01 ... 1.65540096e-01\n",
      "   1.08061150e-01 5.86076788e-02]]\n",
      "\n",
      " [[9.08469881e-01 6.57619059e-01 6.20188065e-01 ... 1.86707100e-01\n",
      "   1.71396771e-01 1.08670388e-01]\n",
      "  [9.64363304e-01 6.57619059e-01 6.20188065e-01 ... 1.86707100e-01\n",
      "   1.71396771e-01 1.08670388e-01]\n",
      "  [9.64363304e-01 9.08469881e-01 6.20188065e-01 ... 1.86707100e-01\n",
      "   1.71396771e-01 1.08670388e-01]\n",
      "  ...\n",
      "  [9.64363304e-01 9.08469881e-01 6.57619059e-01 ... 2.07175339e-01\n",
      "   1.71396771e-01 1.08670388e-01]\n",
      "  [9.64363304e-01 9.08469881e-01 6.57619059e-01 ... 2.07175339e-01\n",
      "   1.86707100e-01 1.08670388e-01]\n",
      "  [9.64363304e-01 9.08469881e-01 6.57619059e-01 ... 2.07175339e-01\n",
      "   1.86707100e-01 1.71396771e-01]]\n",
      "\n",
      " [[8.26437071e-01 7.85652877e-01 7.25864393e-01 ... 3.00023640e-01\n",
      "   2.98513672e-01 4.65865617e-02]\n",
      "  [9.04758138e-01 7.85652877e-01 7.25864393e-01 ... 3.00023640e-01\n",
      "   2.98513672e-01 4.65865617e-02]\n",
      "  [9.04758138e-01 8.26437071e-01 7.25864393e-01 ... 3.00023640e-01\n",
      "   2.98513672e-01 4.65865617e-02]\n",
      "  ...\n",
      "  [9.04758138e-01 8.26437071e-01 7.85652877e-01 ... 3.01715088e-01\n",
      "   2.98513672e-01 4.65865617e-02]\n",
      "  [9.04758138e-01 8.26437071e-01 7.85652877e-01 ... 3.01715088e-01\n",
      "   3.00023640e-01 4.65865617e-02]\n",
      "  [9.04758138e-01 8.26437071e-01 7.85652877e-01 ... 3.01715088e-01\n",
      "   3.00023640e-01 2.98513672e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[7.75621665e-01 7.45191619e-01 5.74337874e-01 ... 8.21297140e-02\n",
      "   4.86670777e-02 2.94835206e-04]\n",
      "  [7.84056749e-01 7.45191619e-01 5.74337874e-01 ... 8.21297140e-02\n",
      "   4.86670777e-02 2.94835206e-04]\n",
      "  [7.84056749e-01 7.75621665e-01 5.74337874e-01 ... 8.21297140e-02\n",
      "   4.86670777e-02 2.94835206e-04]\n",
      "  ...\n",
      "  [7.84056749e-01 7.75621665e-01 7.45191619e-01 ... 9.66969659e-02\n",
      "   4.86670777e-02 2.94835206e-04]\n",
      "  [7.84056749e-01 7.75621665e-01 7.45191619e-01 ... 9.66969659e-02\n",
      "   8.21297140e-02 2.94835206e-04]\n",
      "  [7.84056749e-01 7.75621665e-01 7.45191619e-01 ... 9.66969659e-02\n",
      "   8.21297140e-02 4.86670777e-02]]\n",
      "\n",
      " [[7.32917574e-01 6.24746013e-01 3.66502346e-01 ... 9.07873777e-02\n",
      "   8.65285389e-03 6.42085338e-03]\n",
      "  [8.90056454e-01 6.24746013e-01 3.66502346e-01 ... 9.07873777e-02\n",
      "   8.65285389e-03 6.42085338e-03]\n",
      "  [8.90056454e-01 7.32917574e-01 3.66502346e-01 ... 9.07873777e-02\n",
      "   8.65285389e-03 6.42085338e-03]\n",
      "  ...\n",
      "  [8.90056454e-01 7.32917574e-01 6.24746013e-01 ... 9.23336222e-02\n",
      "   8.65285389e-03 6.42085338e-03]\n",
      "  [8.90056454e-01 7.32917574e-01 6.24746013e-01 ... 9.23336222e-02\n",
      "   9.07873777e-02 6.42085338e-03]\n",
      "  [8.90056454e-01 7.32917574e-01 6.24746013e-01 ... 9.23336222e-02\n",
      "   9.07873777e-02 8.65285389e-03]]\n",
      "\n",
      " [[9.30343407e-01 8.64329022e-01 7.89660733e-01 ... 3.85052472e-01\n",
      "   1.56283035e-01 1.10146011e-01]\n",
      "  [9.91074605e-01 8.64329022e-01 7.89660733e-01 ... 3.85052472e-01\n",
      "   1.56283035e-01 1.10146011e-01]\n",
      "  [9.91074605e-01 9.30343407e-01 7.89660733e-01 ... 3.85052472e-01\n",
      "   1.56283035e-01 1.10146011e-01]\n",
      "  ...\n",
      "  [9.91074605e-01 9.30343407e-01 8.64329022e-01 ... 4.19328667e-01\n",
      "   1.56283035e-01 1.10146011e-01]\n",
      "  [9.91074605e-01 9.30343407e-01 8.64329022e-01 ... 4.19328667e-01\n",
      "   3.85052472e-01 1.10146011e-01]\n",
      "  [9.91074605e-01 9.30343407e-01 8.64329022e-01 ... 4.19328667e-01\n",
      "   3.85052472e-01 1.56283035e-01]]]\n",
      "[3.80577732 4.8086515  5.78991341 5.44705131 4.54350223 5.95431593\n",
      " 6.00542546 7.22233726 6.41132606 5.73687393 3.89644954 6.67278677\n",
      " 4.77831652 5.02798931 5.38815808 5.78255691 5.57277338 6.61257606\n",
      " 4.27306584 5.12230514 6.15371678 4.8986407  5.36184201 6.7614696\n",
      " 5.49649376 5.61335717 5.85480388 6.54636158 5.22536599 5.81578056\n",
      " 6.76823294 5.05829023 5.82225489 3.54349013 6.48086274 2.97661132\n",
      " 6.12238603 7.08464438 5.18980164 6.17590177 6.23260149 5.09182404\n",
      " 7.43258803 5.63505643 5.28011942 5.28265358 6.58249482 4.60346158\n",
      " 4.8379449  4.46268373 5.34499106 5.17323633 6.54312986 3.69608791\n",
      " 6.04988678 5.81404537 4.24325599 5.82601138 5.77452169 4.67970005\n",
      " 3.86685499 6.32111483 5.1894516  6.78810858 4.9480889  4.60806261\n",
      " 4.77706314 6.85968545 5.51616804 5.87993548 6.21677158 6.34813872\n",
      " 4.66731769 4.67543287 5.23255856 7.089116   6.69866922 4.86902681\n",
      " 5.63960765 5.97748744 5.84122348 5.6034835  5.54188161 7.70442928\n",
      " 5.07108575 4.94997125 5.57197329 5.3662216  5.86636274 5.61360517\n",
      " 5.8143594  5.53650605 5.31175394 6.25388262 6.02018875 6.91582039\n",
      " 5.55768121 5.13374285 5.62536526 7.1629665  5.85783575 5.5266889\n",
      " 6.15461359 4.46842668 3.54907012 4.96608392 5.22403988 4.74839157\n",
      " 6.98397675 4.88131071 6.14374373 5.50213631 7.16911355 4.48190306\n",
      " 3.6883454  4.81013148 3.34401218 4.37326452 5.22142307 5.14989506\n",
      " 6.22572611 6.83166431 5.12694445 5.67778624 5.04234983 5.24145996\n",
      " 4.74521753 7.12142754 6.83181353 5.12960166 5.13821184 7.65499459\n",
      " 5.32771154 6.34739364 6.15649688 4.74820812 5.52205434 5.75992578\n",
      " 4.53285925 7.21206176 6.47375674 5.37168625 5.65495681 6.57483702\n",
      " 6.06037752 5.76874833 4.93993254 7.65206906 5.27635608 5.28877404\n",
      " 5.06521217 4.11900233 5.80585943 5.81729825 6.26200676 5.25812528\n",
      " 4.03297145 6.57491047 5.99427062 6.81839837 4.52102006 4.16736581\n",
      " 6.67392476 6.04559484 6.85661607 5.03779578 5.64579691 4.95966693\n",
      " 3.56670248 5.9386613  6.79073105 4.7107431  6.69528926 4.55796932\n",
      " 5.7366811  3.59477797 5.74485174 5.97712368 5.521229   4.43308299\n",
      " 3.97528925 4.99040077 5.4937575  5.89528037 7.23007362 7.13352306\n",
      " 3.58492389 6.67672369 5.45546543 6.26443351 5.03763389 7.08380698\n",
      " 5.82224502 5.47345192 4.43888093 4.19758116 5.29840651 4.30718971\n",
      " 6.16136048 5.45478839 6.87341893 6.13324237 3.78113848 3.78825951\n",
      " 4.93710317 5.70047857 6.3270108  5.79151011 4.02091954 5.38553028\n",
      " 6.97246719 6.51016513 4.67463932 5.93235075 5.74124764 6.5382466\n",
      " 4.23451626 6.19374838 4.15195146 6.38068309 6.36402808 5.49900914\n",
      " 7.53554167 4.99039622 6.25431719 5.85615516 5.73777459 6.13107503\n",
      " 4.44021441 5.13728271 5.68260487 6.28571994 3.96168927 6.7265275\n",
      " 5.22658066 6.76371075 5.13572312 6.787405   4.37575542 6.26505959\n",
      " 4.23387945 5.45580234 4.94540284 4.39154575 4.67268389 6.45727189\n",
      " 6.64338789 6.00752034 4.85693621 4.7291227  6.50176734 6.05117371\n",
      " 6.06489134 6.17241843 3.95873406 6.20260542 6.80503138 6.38312286\n",
      " 6.97217627 6.95725123 5.50230791 5.00263289 4.94088048 4.42032345\n",
      " 6.44982916 6.26481551 5.57963316 5.67280801 4.90002333 5.39225763\n",
      " 6.20361454 4.56156102 6.61642205 3.96012264 5.29250727 4.0583064\n",
      " 6.30506323 5.83585131 5.38924605 4.37965953 5.82331389 6.0686529\n",
      " 5.95172185 5.53530417 4.59058301 5.80815049 4.04331907 7.09635777\n",
      " 5.90496439 6.55656309 6.08749248 5.63980169 4.37535956 5.83275588\n",
      " 4.07202529 6.17064486 5.28000666 4.34626739 7.09162555 4.0743588\n",
      " 6.47190245 5.06109544 5.73775163 4.9742849  4.41004347 5.11954587\n",
      " 4.66313814 4.84006073 5.64401573 4.61116843 6.30279437 5.98037074\n",
      " 5.55733505 4.08097582 4.53450124 5.32584469 4.99559773 5.01077929\n",
      " 5.81080887 5.30575472 4.98949642 5.52320054 5.46543369 4.18139222\n",
      " 4.69265669 4.82673382 6.57691636 3.18879577 6.16360201 4.88069367\n",
      " 5.07676686 4.41334115 5.68729667 4.43811822 4.50425122 8.04597343\n",
      " 6.04305033 4.87571911 5.95725255 7.17767049 6.10298568 5.57046555\n",
      " 6.00744006 6.7392514  5.34122136 4.26766445 5.69572951 5.86817512\n",
      " 5.86375297 6.00042332 6.55938717 5.33076513 5.28708013 5.17143324\n",
      " 4.35040176 7.39364171 5.33874656 4.93517112 5.5527412  5.88046723\n",
      " 5.90366347 6.63601534 3.25345927 5.7719547  5.70864041 4.47670757\n",
      " 5.29679955 6.50413705 5.48853714 6.87003114 5.70814528 5.72223111\n",
      " 6.23575439 4.7722277  5.46397087 6.08399633 4.96631225 6.97995631\n",
      " 4.78170868 5.76923598 4.08128039 6.31380326 4.78995983 5.51883244\n",
      " 5.69067212 4.47853342 6.05076034 5.29847386 4.9439492  6.42505964\n",
      " 5.27561088 4.22075514 5.30881881 5.28579011 6.22276698 6.11060557\n",
      " 4.89248651 5.01389013 5.38651848 6.1552009  6.48118074 4.45419505\n",
      " 5.2610499  4.73280947 5.82306206 6.12666842 4.0308958  4.62618775\n",
      " 5.80943054 5.49057913 4.35875656 5.84283509 5.93820424 5.47959293\n",
      " 4.60338802 5.47243841 6.81180652 6.58632133 5.89547719 5.34153147\n",
      " 5.7078852  4.63068261 5.69411143 5.35798388 5.4233091  6.55479278\n",
      " 4.28409264 5.68499945 4.49672739 5.217647   5.84443646 5.04660339\n",
      " 4.93087562 4.46377631 5.91193297 3.30046592 4.47533646 4.25969029\n",
      " 5.56080659 5.0649079  6.09468698 4.86288095 6.22536218 5.28782581\n",
      " 6.48753497 4.60970966 5.01862539 5.24680049 6.77845632 5.9124951\n",
      " 5.18378863 6.46683668 7.15854933 6.23030013 5.73849126 5.39168122\n",
      " 5.08898796 4.33151087 6.1083611  5.24115663 4.23115136 6.51775511\n",
      " 5.54327575 6.23883223 4.27476931 5.06097575 5.45588366 5.57434081\n",
      " 6.15218211 4.72269677 4.55105505 5.22586784 7.76956841 7.01930435\n",
      " 3.13979996 4.39033513 4.21233423 4.41932661 5.64446461 5.17900898\n",
      " 6.64648832 5.85415293 6.19590991 4.19962773 4.26531236 6.57173599\n",
      " 6.09681684 4.04467489 4.51262598 4.37725151 4.3123335  5.92325795\n",
      " 6.41516293 4.41960796 5.67145209 5.05975976 5.97747918 5.95082225\n",
      " 4.72604558 6.03552819 4.39686587 7.13252902 6.50339457 5.01465588\n",
      " 5.75027878 5.68894429 5.89529869 5.88580437 5.17596496 5.8618874\n",
      " 5.42845326 6.36376296 7.00958059 4.06883129 6.21369539 7.74510989\n",
      " 4.63611483 4.08944133 5.38926182 4.86955133 7.00525212 4.2747986\n",
      " 5.49341942 5.52329128 5.86101367 6.04489079 6.13120617 5.3258584\n",
      " 5.56873961 4.76718835 4.28470968 5.37535179 7.12890975 6.11173222\n",
      " 5.97226412 4.9696429  7.1854419  5.09769537 5.80296575 5.96430271\n",
      " 4.40063879 4.75047737 5.90458137 3.86596081 5.34280836 6.06264513\n",
      " 5.352839   4.69889813 5.91574885 4.09960481 6.15306132 5.05058261\n",
      " 6.271511   4.11574188 5.9439946  5.5724415  7.35776009 5.0930198\n",
      " 3.80476135 5.51994944 4.44126873 6.71382036 5.66657929 5.15537573\n",
      " 3.45740429 5.207339   4.26141241 5.72347548 6.66294301 4.2901132\n",
      " 4.83658404 6.65361438 6.91198172 3.98579713 6.81455158 5.3938733\n",
      " 3.23911229 4.56293105 5.16006383 5.91195786 4.64410629 5.23316484\n",
      " 4.40487478 5.89491578 5.18667242 5.65999972 6.15619506 6.76289374\n",
      " 5.77634042 6.07212482 4.94456998 5.61475024 5.13070061 5.01480802\n",
      " 4.69387199 5.14099729 5.28996375 6.33820471 4.80309772 5.74008291\n",
      " 6.02904375 5.52226563 5.0079065  6.09194559 7.00739617 4.25377195\n",
      " 4.45948764 5.61372646 4.75642742 5.56434704 5.14131311 5.61877318\n",
      " 5.71608871 3.71425193 4.79565139 6.09747209 6.47673814 6.85875935\n",
      " 6.05826229 3.30973707 5.31819749 4.91989206 4.31044874 4.86852527\n",
      " 5.22159834 4.79855593 5.79376838 3.31730386 6.88789888 4.73424584\n",
      " 5.00323626 5.82206922 6.12749961 5.59165836 6.31912479 6.37475526\n",
      " 4.97014386 5.44933987 3.80811785 6.43124934 4.55666483 5.16526047\n",
      " 3.28323471 3.83747482 5.39352048 5.01403301 4.22324122 3.56021126\n",
      " 6.4971353  4.29196219 5.79195197 6.64913871 6.45753406 5.65834707\n",
      " 4.47319667 5.2956271  5.10333027 5.06479272 6.00536939 4.50444669\n",
      " 4.99725195 6.15736577 5.3951267  5.15749353 6.87225134 6.52317336\n",
      " 5.1937682  5.20127303 4.6713655  4.62250598 5.68819836 5.34721646\n",
      " 6.87551473 6.03463393 5.15319373 4.42535358 5.57566583 4.94106571\n",
      " 6.11903216 5.45930486 5.69921368 5.6013846  4.77346196 5.51201718\n",
      " 4.76155959 6.09495479 4.78260591 4.97024499 6.02333026 4.09215212\n",
      " 4.67622167 7.09694552 6.26330742 4.77189429 6.97178113 4.7015013\n",
      " 5.18899783 5.24282269 5.67418553 5.19709389 7.31016274 6.06976082\n",
      " 3.81184524 3.68040357 6.6235872  5.07335923 5.61502895 6.97690106\n",
      " 4.35844055 4.8091182  5.86003867 4.99065951 4.8021391  4.25352787\n",
      " 3.84261512 5.31609212 5.13403291 4.23278639 4.88670658 4.7377829\n",
      " 5.89096037 7.19729124 6.34929835 4.89294924 5.49929104 5.74809892\n",
      " 6.15198136 7.65334541 5.6247757  6.60847706 6.36859058 5.06007065\n",
      " 5.58816548 5.74199847 5.39255275 7.38197525 6.75762562 6.82667167\n",
      " 6.51243926 5.9097525  4.75267536 5.8859682  6.13992001 5.15618512\n",
      " 5.85901007 7.45485184 5.9586483  5.21234675 4.94822214 4.53325618\n",
      " 6.60409652 4.59846989 6.45468597 6.30858562 5.98089923 4.89285283\n",
      " 5.98569666 5.23691107 7.80246877 4.79572571 5.63243216 4.39942063\n",
      " 5.00611328 6.36551513 6.30600852 4.73253661 5.19978338 6.1656728\n",
      " 5.63076046 6.11870338 5.89105084 4.84519314 5.57814122 5.62583619\n",
      " 6.01056127 5.70701458 5.46592926 6.58896994 7.44377283 6.37099329\n",
      " 6.53307377 6.49912965 5.06089975 6.38613706 4.24216178 5.86792712\n",
      " 3.18818538 6.03030111 5.06795951 5.52275048 5.66318853 7.39098349\n",
      " 6.29503997 7.3446226  5.76371671 6.88574108 6.83410903 5.98462724\n",
      " 3.6828498  4.80593898 6.00710967 5.83544028 5.41284469 5.66713553\n",
      " 5.98787082 5.2939644  4.85736406 4.00170937 4.59884412 5.06283289\n",
      " 5.95723699 4.38525158 4.67136338 4.40175249 4.14617321 6.30101135\n",
      " 4.57382932 4.85309983 5.60570861 5.0839593  7.35229476 2.86154904\n",
      " 4.96309112 6.00314546 4.10122639 5.18080415 6.23646414 7.72554328\n",
      " 6.41125472 4.69220414 7.07942193 5.11885009 4.99253044 7.48541629\n",
      " 5.46927423 5.2484228  4.8142131  4.40339417 5.74899784 5.45495286\n",
      " 5.45738321 5.67870571 6.65813564 4.5537302  6.49775007 6.41991504\n",
      " 6.21990356 5.90432521 6.45421627 5.02970262 4.09092505 5.02643877\n",
      " 4.27068027 5.73331184 5.87901859 3.86896539 3.94446833 4.78745175\n",
      " 5.64323425 4.65813249 3.9317742  3.504208   6.63058831 7.55523231\n",
      " 7.53339517 5.69066547 6.52619702 5.33459311 4.76617308 4.43834193\n",
      " 7.14072809 4.11202243 6.35094532 5.36917454 3.10129004 6.63664695\n",
      " 4.27032524 4.81821669 7.93279972 5.76534843 6.51642054 5.31875075\n",
      " 5.36319557 5.95344069 6.18942689 6.41791718 4.0990815  4.72105479\n",
      " 3.52981993 6.46053189 5.47608878 6.61494478 4.87805171 4.87698023\n",
      " 4.60976743 6.45885306 4.68405254 6.04583955 5.65837335 5.14262675\n",
      " 4.29603027 4.88171869 3.98395678 6.47716815 5.27244677 5.38802576\n",
      " 4.60036047 6.21664308 6.1770064  7.62743466 5.96347581 8.21401891\n",
      " 5.52035057 3.30860172 5.2668636  5.89017556 4.92087816 5.676517\n",
      " 5.56139142 4.69479038 5.04315665 3.93236306 5.19466876 4.2171773\n",
      " 5.41522698 4.77323763 6.47072134 7.04961878 4.67979209 5.15951627\n",
      " 4.9915114  4.99748357 6.94876363 6.88265895 6.16855989 5.27770355\n",
      " 7.88619495 7.47538096 5.67998444 5.80427921 4.41013989 4.78753739\n",
      " 5.0425011  6.15557122 5.01028038 4.22299235 7.97026561 6.29365602\n",
      " 5.92883348 5.64770312 4.94038471 3.48130666 5.60738234 2.69329655\n",
      " 5.37655086 5.83516521 5.78718571 5.76463469 7.00950505 7.13453517\n",
      " 6.51778881 5.69287984 4.80253856 5.80482301 5.61151554 5.30836497\n",
      " 5.65085579 4.08016329 6.26381711 4.84493548 5.42117899 6.52698881\n",
      " 6.67775205 5.48009107 5.41433489 4.64018491 5.11662488 5.01409789\n",
      " 5.3192254  5.72579494 5.0074306  6.29705201 6.03601802 3.18467065\n",
      " 5.60056999 7.50831896 5.59347839 4.99120489 6.41919971 5.88569063\n",
      " 5.55705184 5.80610289 5.20678226 5.33710817 6.77675902 5.52954963\n",
      " 4.88532299 4.54818626 3.46775895 6.51628709]\n"
     ]
    }
   ],
   "source": [
    "def h_3_star(a, b, t):\n",
    "    return a - min(a, t) + b - min(b, t) + max(min(a, t)+min(b, t), 2*t/3) + 1/2 * max(min(a, t)+min(b, t), t) - 1/2 * max(max(min(a, t), min(b, t)), 2*t/3) - t/6\n",
    "\n",
    "\n",
    "def f_function(a, b, z):\n",
    "    if(z >= 1):\n",
    "        return (a+b)/2 + z/3\n",
    "    else:\n",
    "        return z/3 + h_3_star(a, b, 1-z)/2\n",
    "\n",
    "def h_function_label(input_list):\n",
    "    #input_list = sorted(input_list)\n",
    "    g_list = []\n",
    "    for j1 in range(len(input_list) ):\n",
    "        for j2 in range(len(input_list)):\n",
    "            if(j1 != j2):\n",
    "                a = input_list[j1]\n",
    "                b = input_list[j2]\n",
    "                z = sum(input_list)- a-b\n",
    "\n",
    "                g_list.append( f_function(a, b, z) * (Agent_number_n-1))\n",
    "    h = sum(g_list) * 3 /  (Agent_number_n) /  (Agent_number_n-1) /  (Agent_number_n - 2)\n",
    "    return h\n",
    "\n",
    "def appen(_x_list,y):\n",
    "    global temp_list\n",
    "    temp_list.append(_x_list)\n",
    "    \n",
    "def appen_train(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "        \n",
    "        h = h_function_label(temp)\n",
    "        h_list.append(float(h))\n",
    "    temp_list = np.array(temp_list)\n",
    "    x_list = np.array(x_list)\n",
    "    return temp_list,S,x_list,h_list\n",
    "    \n",
    "\n",
    "def appen_test(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    testing_data.append(temp_list)\n",
    "    testing_label.append(S)\n",
    "    temp_list = np.array(temp_list)\n",
    "    return temp_list,S\n",
    "    \n",
    "\n",
    "def read_testing_data():\n",
    "    for i in range(1000):\n",
    "        appen_test(Generate_distribution(Agent_number_n));\n",
    "                        \n",
    "            \n",
    "\n",
    "testing_data=[]\n",
    "testing_label=[]\n",
    "S=1.0\n",
    "read_testing_data();\n",
    "\n",
    "testing_data=np.array(testing_data)\n",
    "testing_label=np.array(testing_label)\n",
    "print(testing_data)\n",
    "print(testing_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:14.018650Z",
     "start_time": "2021-06-27T04:56:13.989727Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.normal_(m.bias, mean=0.0, std=0.01)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.input_dim = new_input# (Agent_number_n-1)\n",
    "        self.hidden_dim = 128\n",
    "        self.output_dim = 1\n",
    "        self.hidden_layer_count = 6 \n",
    "        \n",
    "        current_dim = self.input_dim\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(self.hidden_layer_count):\n",
    "            self.layers.append(torch.nn.Linear(current_dim, self.hidden_dim))\n",
    "            current_dim = self.hidden_dim\n",
    "        self.layers.append(torch.nn.Linear(current_dim, self.output_dim))\n",
    "\n",
    "    def calculate(self, value_list):\n",
    "        h = Dimensionality_reduction(value_list)\n",
    "        for layer in self.layers:\n",
    "            h = torch.relu(layer(h))\n",
    "        return h\n",
    "\n",
    "    def forward(self, input_list,input_label,list_x):\n",
    "        global iteration,echo,target_order\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        input_list = torch.from_numpy(\n",
    "            np.array(input_list)).to(dev).type(torch.float32)\n",
    "        h_list = []\n",
    "\n",
    "        for i in range(Agent_number_n):\n",
    "            h = self.calculate(input_list[i])\n",
    "            h_list.append(h)\n",
    "#             loss3 += torch.square(h_function_2(input_list)-h2[1])\n",
    "            \n",
    "        input_label = torch.from_numpy(\n",
    "            np.array(input_label)).to(dev).type(torch.float32)\n",
    "        sum_h = torch.sum(torch.cat(h_list)).to(dev)\n",
    "\n",
    "\n",
    "        loss1 = torch.where((Agent_number_n-1)*input_label>sum_h,\n",
    "                        torch.square(((Agent_number_n-1)*input_label-sum_h)),\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "        loss2 = torch.where((Agent_number_n-Alpha)*input_label<sum_h,\n",
    "                        torch.square((sum_h-(Agent_number_n-Alpha)*input_label))/100,\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "\n",
    "        return loss1,loss2\n",
    "    \n",
    "    def supervised_loss(self, input_list,label):\n",
    "        global iteration,echo,target_order\n",
    "        input_list = torch.from_numpy(\n",
    "            np.array(input_list)).to(dev).type(torch.float32)\n",
    "        loss = 0 \n",
    "        for i in range(Agent_number_n):\n",
    "            h = self.calculate(input_list[i])\n",
    "            loss += torch.square(h - label[i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:14.257011Z",
     "start_time": "2021-06-27T04:56:14.020644Z"
    }
   },
   "outputs": [],
   "source": [
    "def redistribution_value_function(input_tensor):\n",
    "    S = torch.max(torch.sum(input_tensor), torch.ones(1).to(dev))\n",
    "    temp_list = []\n",
    "\n",
    "\n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        \n",
    "        for j in range(Agent_number_n):\n",
    "            if(i != j):\n",
    "                temp .append(input_tensor[j])\n",
    "                \n",
    "        temp = torch.stack(temp)\n",
    "        temp_list.append(temp)\n",
    "    return torch.stack(temp_list), S\n",
    "\n",
    "GeneratorNet = nn.Sequential(                      # Generator\n",
    "    # random ideas (could from normal distribution)\n",
    "    nn.Linear(N_IDEAS, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    # making a painting from these random ideas\n",
    "    nn.Linear(64, ART_COMPONENTS),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:23.761879Z",
     "start_time": "2021-06-27T04:56:14.260006Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "random.seed(2000)\n",
    "torch.manual_seed(256)\n",
    "DiscriminatorNet  = Net()\n",
    "DiscriminatorNet.apply(weight_init)\n",
    "GeneratorNet.apply(weight_init)\n",
    "# DiscriminatorNet = torch.load(\"save/Deep_learning_D_11_supervised_1\")\n",
    "# if(Is_GAN):\n",
    "#     GeneratorNet = torch.load(\"save/Deep_learning_G_11\")\n",
    "DiscriminatorNet.to(dev)\n",
    "GeneratorNet.to(dev)\n",
    "\n",
    "opt_D = torch.optim.Adam(DiscriminatorNet.parameters(), lr=LR_D)\n",
    "opt_G = torch.optim.Adam(GeneratorNet.parameters(), lr=LR_G)\n",
    "\n",
    "\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(opt_D, step_size=100, gamma=0.98)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(opt_G, step_size=100, gamma=0.98)\n",
    "\n",
    "index_train_list = []\n",
    "index_test_list = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:23.776773Z",
     "start_time": "2021-06-27T04:56:23.763809Z"
    }
   },
   "outputs": [],
   "source": [
    "#supervised\n",
    "index_supervisedtrain_list = []\n",
    "supervisedtrain_losses = []\n",
    "for iteration in range(echo):\n",
    "\n",
    "    temp_number = 0\n",
    "    total_batch_loss = 0 \n",
    "    \n",
    "    loss_sum = 0\n",
    "    denominator = 0\n",
    "    for index in range(0, BATCH_SIZE):\n",
    "        training_data_i, training_S, training_data,label = appen_train(\n",
    "            Generate_distribution(Agent_number_n))\n",
    "        h_loss = DiscriminatorNet.supervised_loss(training_data_i, label)\n",
    "        denominator += 1\n",
    "        loss_sum += h_loss\n",
    "\n",
    "    loss_sum = torch.sum(loss_sum)\n",
    "    loss = (loss_sum) / denominator \n",
    "    total_batch_loss += float(loss_sum)\n",
    "\n",
    "    opt_D.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_D.step()\n",
    "\n",
    "    if (iteration%100 == 0):\n",
    "        print(iteration,loss,total_batch_loss)\n",
    "        index_supervisedtrain_list.append(iteration)\n",
    "        supervisedtrain_losses.append(total_batch_loss)\n",
    "    scheduler_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:23.999309Z",
     "start_time": "2021-06-27T04:56:23.777770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ylim(0, 0.1)\n",
    "plt.plot(index_supervisedtrain_list ,supervisedtrain_losses)\n",
    "plt.ylabel('supervised train loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.092249Z",
     "start_time": "2021-06-27T04:56:24.000177Z"
    }
   },
   "outputs": [],
   "source": [
    "if(echo!=0):\n",
    "    torch.save(DiscriminatorNet, \"save/Deep_learning_D_11_supervised\")\n",
    "    if(Is_GAN):\n",
    "        torch.save(GeneratorNet, \"save/Deep_learning_G_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.107250Z",
     "start_time": "2021-06-27T04:56:24.093222Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_D = torch.optim.Adam(DiscriminatorNet.parameters(), lr=LR_D/10)\n",
    "opt_G = torch.optim.Adam(GeneratorNet.parameters(), lr=LR_G/100)\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(opt_D, step_size=100, gamma=0.98)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(opt_G, step_size=100, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-27T04:57:57.335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Gan: tensor(-0., device='cuda:0', grad_fn=<MaxBackward1>) tensor([100.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(int(echo)):\n",
    "\n",
    "    temp_number = 0\n",
    "    total_batch_loss = 0 \n",
    "    \n",
    "    loss2_list = []\n",
    "    loss1_sum = 0\n",
    "    loss2_sum = 0\n",
    "    loss = 0\n",
    "    if(Is_GAN==False):\n",
    "        denominator = 0\n",
    "        for index in range(0, BATCH_SIZE):\n",
    "            training_data_i, training_label, training_data, label = appen_train(\n",
    "                Generate_distribution(Agent_number_n))\n",
    "            h_loss1, h_loss2 = DiscriminatorNet(training_data_i, training_label,\n",
    "                                           training_data)\n",
    "            denominator += 1\n",
    "            loss1_sum += h_loss1\n",
    "            loss2_sum += h_loss2\n",
    "\n",
    "        loss_sum = torch.sum(loss1_sum + loss2_sum)\n",
    "        loss = (loss_sum) / denominator \n",
    "        total_batch_loss +=float(loss_sum)\n",
    "\n",
    "        opt_D.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "        temp_number = iteration\n",
    "        index_train_list.append(iteration)\n",
    "        train_losses.append(total_batch_loss)\n",
    "\n",
    "\n",
    "    ## Gan \n",
    "    else:## Gan Work traning GeneratorNet\n",
    "\n",
    "        # real painting from artist\n",
    "        G_ideas = torch.randn(BATCH_SIZE, N_IDEAS,\n",
    "                              requires_grad=True).to(dev)  # random ideas\\n\n",
    "        # fake painting from G (random ideas)\n",
    "\n",
    "        G_values = GeneratorNet(G_ideas)\n",
    "        G_values , indices = torch.sort(G_values, descending=True)\n",
    "    #     print(artist_paintings)\n",
    "    #     print(G_paintings)\n",
    "\n",
    "        result_list = []\n",
    "        for index in range(BATCH_SIZE):\n",
    "            h_list = []\n",
    "            value_list_tensor, S_tensor = redistribution_value_function(\n",
    "                G_values[index])\n",
    "            for i in range(Agent_number_n):\n",
    "                h = DiscriminatorNet.calculate(\n",
    "                    value_list_tensor[i].cuda().type(torch.float32))\n",
    "                h_list.append(h)\n",
    "            h_list = torch.stack(h_list)\n",
    "            result_list.append(torch.sum(h_list)/S_tensor.cuda())\n",
    "        result_list = torch.stack(result_list)\n",
    "\n",
    "        diff_loss = torch.max(result_list)-torch.min(result_list)\n",
    "        G_loss = torch.max(- diff_loss)\n",
    "\n",
    "        opt_G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "    # real painting from artist\n",
    "        G_ideas = torch.randn(BATCH_SIZE, N_IDEAS,\n",
    "                              requires_grad=True).to(dev)  # random ideas\\n\n",
    "        # fake painting from G (random ideas)\n",
    "\n",
    "        G_values = GeneratorNet(G_ideas)\n",
    "        G_values , indices = torch.sort(G_values, descending=True)\n",
    "    #     print(artist_paintings)\n",
    "    #     print(G_paintings)\n",
    "\n",
    "        result_list = []\n",
    "        for index in range(BATCH_SIZE):\n",
    "            h_list = []\n",
    "            value_list_tensor, S_tensor = redistribution_value_function(\n",
    "                G_values[index])\n",
    "            for i in range(Agent_number_n):\n",
    "                h = DiscriminatorNet.calculate(\n",
    "                    value_list_tensor[i].cuda().type(torch.float32))\n",
    "                h_list.append(h)\n",
    "            h_list = torch.stack(h_list)\n",
    "            result_list.append(torch.sum(h_list)/S_tensor.cuda())\n",
    "        result_list = torch.stack(result_list)\n",
    "\n",
    "        diff_loss = torch.max(result_list)-torch.min(result_list)\n",
    "\n",
    "        D_loss = torch.where((Agent_number_n-1)>torch.min(result_list),\n",
    "            torch.square(((Agent_number_n-1)-torch.min(result_list))),\n",
    "            torch.zeros(1).to(dev)\n",
    "          )   + torch.where((Agent_number_n-Alpha)<torch.max(result_list),\n",
    "                        torch.square((torch.max(result_list)-(Agent_number_n-Alpha)))/10000,\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "\n",
    "        opt_D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "\n",
    "\n",
    "    scheduler_D.step()\n",
    "    scheduler_G.step()\n",
    "    temp_number = iteration\n",
    "    if (iteration%50 == 0):\n",
    "        print(temp_number)\n",
    "        if(Is_GAN):\n",
    "            print(\"Gan:\",G_loss,D_loss)\n",
    "            print()\n",
    "        else:\n",
    "            print(loss,float(loss1_sum),float(loss2_sum))\n",
    "            print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.141093Z",
     "start_time": "2021-06-27T04:56:05.916Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylim(0, 1.0)\n",
    "plt.plot(index_test_list,test_losses)\n",
    "plt.ylabel('test loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.142090Z",
     "start_time": "2021-06-27T04:56:05.917Z"
    }
   },
   "outputs": [],
   "source": [
    "if(echo!=0):\n",
    "    torch.save(DiscriminatorNet, \"save/Deep_learning_D_11_supervised_1\")\n",
    "    if(Is_GAN):\n",
    "        torch.save(GeneratorNet, \"save/Deep_learning_G_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.143088Z",
     "start_time": "2021-06-27T04:56:05.918Z"
    }
   },
   "outputs": [],
   "source": [
    "denominator = 0\n",
    "result_list = []\n",
    "for index in range(len(testing_data)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        h = DiscriminatorNet.calculate(torch.tensor(testing_data[index][i]).to(dev).type(torch.float32))\n",
    "        h_list.append(float(h))\n",
    "    \n",
    "    #print(sum(h_list),h_list,sum(h_list)/testing_label[index])\n",
    "    result_list.append(sum(h_list)/testing_label[index])\n",
    "    \n",
    "\n",
    "print(max(result_list),min(result_list),max(result_list)-min(result_list))\n",
    "print(sum(result_list)/len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.144085Z",
     "start_time": "2021-06-27T04:56:05.919Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list)/len(result_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.145081Z",
     "start_time": "2021-06-27T04:56:05.920Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def appen_test_G(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    testing_data_G.append(temp_list)\n",
    "    testing_label_G.append(S)\n",
    "def read_testing_data_G():\n",
    "    for i in range(10000):\n",
    "        #appen_test_G(sorted(np.random.rand(Agent_number_n), reverse=True));\n",
    "        G_ideas = torch.randn(N_IDEAS).to(dev)  # random ideas\\n\n",
    "        G_values = GeneratorNet(G_ideas)\n",
    "        G_values , indices = torch.sort(G_values, descending=True)\n",
    "        appen_test_G(G_values.detach().cpu().numpy()) \n",
    "\n",
    "testing_data_G = []\n",
    "testing_label_G = []\n",
    "read_testing_data_G()\n",
    "testing_data_G=np.array(testing_data_G)\n",
    "testing_label_G=np.array(testing_label_G)\n",
    "print(testing_data_G)\n",
    "print(testing_label_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.146080Z",
     "start_time": "2021-06-27T04:56:05.921Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "result_list_G = []\n",
    "for index in range(len(testing_data_G)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        h = DiscriminatorNet.calculate(torch.tensor(testing_data_G[index][i]).to(dev).type(torch.float32))\n",
    "        h_list.append(float(h))\n",
    "    \n",
    "    #print(sum(h_list),h_list,sum(h_list)/testing_label[index])\n",
    "    result_list_G.append(sum(h_list)/testing_label_G[index])\n",
    "    \n",
    "\n",
    "print(max(result_list_G),min(result_list_G),max(result_list_G)-min(result_list_G))\n",
    "print(sum(result_list_G)/len(result_list_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.147076Z",
     "start_time": "2021-06-27T04:56:05.922Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list_G,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list_G)/len(result_list_G), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list_G.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list_G)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list_G)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T04:56:24.148074Z",
     "start_time": "2021-06-27T04:56:05.923Z"
    }
   },
   "outputs": [],
   "source": [
    "final_list = result_list + result_list_G\n",
    "\n",
    "\n",
    "plt.hist(final_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(final_list)/len(final_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "final_list.sort()\n",
    "\n",
    "plt.axvline(x=final_list[int(len(final_list)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=final_list[int(len(final_list)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
