{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T05:50:01.841559Z",
     "start_time": "2021-06-14T05:49:59.588324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pygame\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as opt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats as st\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from matplotlib.colors import LogNorm \n",
    "import matplotlib.cm as cm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy.interpolate import griddata\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "\n",
    "print(dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T05:50:01.857027Z",
     "start_time": "2021-06-14T05:50:01.843554Z"
    }
   },
   "outputs": [],
   "source": [
    "global temp_list\n",
    "temp_list = []\n",
    "Agent_number_n=10;\n",
    "Alpha = 0.881 \n",
    "\n",
    "# Hyper Parameters\n",
    "echo = 2001 \n",
    "BATCH_SIZE = 64\n",
    "LR_G = 0.001           # learning rate for generator\n",
    "LR_D = 0.001           # learning rate for discriminator\n",
    "N_IDEAS = Agent_number_n             # think of this as number of ideas for generating an art work (Generator)\n",
    "ART_COMPONENTS = Agent_number_n     # it could be total point G can draw in the canvas\n",
    "\n",
    "Is_GAN = True # if use Gan\n",
    "\n",
    "def Generate_distribution(Agent_number_n):\n",
    "    return sorted(np.random.rand(Agent_number_n), reverse=True)\n",
    "    #return sorted(np.random.normal(normalloc,normalscale,Agent_number_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T05:50:02.094828Z",
     "start_time": "2021-06-14T05:50:01.858399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[8.27358562e-01 8.17117827e-01 7.49588829e-01]\n",
      "  [9.52238981e-01 8.17117827e-01 7.49588829e-01]\n",
      "  [9.52238981e-01 8.27358562e-01 7.49588829e-01]\n",
      "  [9.52238981e-01 8.27358562e-01 8.17117827e-01]]\n",
      "\n",
      " [[4.26504562e-01 3.96084225e-01 6.63784283e-02]\n",
      "  [6.27211053e-01 3.96084225e-01 6.63784283e-02]\n",
      "  [6.27211053e-01 4.26504562e-01 6.63784283e-02]\n",
      "  [6.27211053e-01 4.26504562e-01 3.96084225e-01]]\n",
      "\n",
      " [[7.78907050e-01 5.10250865e-02 1.00261766e-04]\n",
      "  [8.46977303e-01 5.10250865e-02 1.00261766e-04]\n",
      "  [8.46977303e-01 7.78907050e-01 1.00261766e-04]\n",
      "  [8.46977303e-01 7.78907050e-01 5.10250865e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[3.72699128e-01 3.67230405e-01 2.87194734e-01]\n",
      "  [5.00837294e-01 3.67230405e-01 2.87194734e-01]\n",
      "  [5.00837294e-01 3.72699128e-01 2.87194734e-01]\n",
      "  [5.00837294e-01 3.72699128e-01 3.67230405e-01]]\n",
      "\n",
      " [[7.47876086e-01 6.22221995e-01 5.39502388e-02]\n",
      "  [9.00303078e-01 6.22221995e-01 5.39502388e-02]\n",
      "  [9.00303078e-01 7.47876086e-01 5.39502388e-02]\n",
      "  [9.00303078e-01 7.47876086e-01 6.22221995e-01]]\n",
      "\n",
      " [[3.84545937e-01 1.89380635e-01 8.95650527e-02]\n",
      "  [9.64268622e-01 1.89380635e-01 8.95650527e-02]\n",
      "  [9.64268622e-01 3.84545937e-01 8.95650527e-02]\n",
      "  [9.64268622e-01 3.84545937e-01 1.89380635e-01]]]\n",
      "[3.3463042  1.51617827 1.6770097  ... 1.52796156 2.3243514  1.62776025]\n"
     ]
    }
   ],
   "source": [
    "def h_3_star(a, b, t):\n",
    "    return a - min(a, t) + b - min(b, t) + max(min(a, t)+min(b, t), 2*t/3) + 1/2 * max(min(a, t)+min(b, t), t) - 1/2 * max(max(min(a, t), min(b, t)), 2*t/3) - t/6\n",
    "\n",
    "\n",
    "def f_function(a, b, z):\n",
    "    if(z >= 1):\n",
    "        return (a+b)/2 + z/3\n",
    "    else:\n",
    "        return z/3 + h_3_star(a, b, 1-z)/2\n",
    "\n",
    "def h_function_label(input_list):\n",
    "    #input_list = sorted(input_list)\n",
    "    g_list = []\n",
    "    for j1 in range(len(input_list) ):\n",
    "        for j2 in range(len(input_list)):\n",
    "            if(j1 != j2):\n",
    "                a = input_list[j1]\n",
    "                b = input_list[j2]\n",
    "                z = sum(input_list)- a-b\n",
    "\n",
    "                g_list.append( f_function(a, b, z) * (Agent_number_n-1))\n",
    "    h = sum(g_list) * 3 /  (Agent_number_n) /  (Agent_number_n-1) /  (Agent_number_n - 2)\n",
    "    return h\n",
    "\n",
    "def appen(_x_list,y):\n",
    "    global temp_list\n",
    "    temp_list.append(_x_list)\n",
    "    \n",
    "def appen_train(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "        \n",
    "        h = h_function_label(temp)\n",
    "        h_list.append(float(h))\n",
    "    temp_list = np.array(temp_list)\n",
    "    x_list = np.array(x_list)\n",
    "    return temp_list,S,x_list,h_list\n",
    "    \n",
    "\n",
    "def appen_test(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    testing_data.append(temp_list)\n",
    "    testing_label.append(S)\n",
    "    temp_list = np.array(temp_list)\n",
    "    return temp_list,S\n",
    "    \n",
    "\n",
    "def read_testing_data():\n",
    "    for i in range(10000):\n",
    "        appen_test(Generate_distribution(Agent_number_n));\n",
    "                            \n",
    "\n",
    "testing_data=[]\n",
    "testing_label=[]\n",
    "S=1.0\n",
    "read_testing_data();\n",
    "\n",
    "testing_data=np.array(testing_data)\n",
    "testing_label=np.array(testing_label)\n",
    "print(testing_data)\n",
    "print(testing_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T05:50:02.125749Z",
     "start_time": "2021-06-14T05:50:02.096761Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.input_dim = (Agent_number_n-1)\n",
    "        self.hidden_dim = 128\n",
    "        self.output_dim = 1\n",
    "        self.hidden_layer_count = 6 \n",
    "        \n",
    "        current_dim = self.input_dim\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(self.hidden_layer_count):\n",
    "            self.layers.append(torch.nn.Linear(current_dim, self.hidden_dim))\n",
    "            current_dim = self.hidden_dim\n",
    "        self.layers.append(torch.nn.Linear(current_dim, self.output_dim))\n",
    "\n",
    "    def calculate(self, value_list):\n",
    "        h = value_list\n",
    "        LeakyReLU = torch.nn.LeakyReLU()\n",
    "        for layer in self.layers:\n",
    "            h = torch.relu(layer(h))\n",
    "        return h\n",
    "\n",
    "    def forward(self, input_list,input_label,list_x):\n",
    "        global iteration,echo,target_order\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        input_list = torch.from_numpy(\n",
    "            np.array(input_list)).to(dev).type(torch.float32)\n",
    "        h_list = []\n",
    "\n",
    "        for i in range(Agent_number_n):\n",
    "            h = self.calculate(input_list[i])\n",
    "            h_list.append(h)\n",
    "#             loss3 += torch.square(h_function_2(input_list)-h2[1])\n",
    "            \n",
    "        input_label = torch.from_numpy(\n",
    "            np.array(input_label)).to(dev).type(torch.float32)\n",
    "        sum_h = torch.sum(torch.cat(h_list)).to(dev)\n",
    "\n",
    "\n",
    "        loss1 = torch.where((Agent_number_n-1)*input_label>sum_h,\n",
    "                        torch.square(((Agent_number_n-1)*input_label-sum_h)),\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "        loss2 = torch.where((Agent_number_n-Alpha)*input_label<sum_h,\n",
    "                        torch.square((sum_h-(Agent_number_n-Alpha)*input_label))/100,\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "\n",
    "        return loss1,loss2\n",
    "    \n",
    "    def supervised_loss(self, input_list,label):\n",
    "        global iteration,echo,target_order\n",
    "        input_list = torch.from_numpy(\n",
    "            np.array(input_list)).to(dev).type(torch.float32)\n",
    "        loss = 0 \n",
    "        for i in range(Agent_number_n):\n",
    "            h = self.calculate(input_list[i])\n",
    "            loss += torch.square(h - label[i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T05:50:02.141705Z",
     "start_time": "2021-06-14T05:50:02.127743Z"
    }
   },
   "outputs": [],
   "source": [
    "def redistribution_value_function(input_tensor):\n",
    "    S = torch.max(torch.sum(input_tensor), torch.ones(1).to(dev))\n",
    "    temp_list = []\n",
    "\n",
    "\n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        \n",
    "        for j in range(Agent_number_n):\n",
    "            if(i != j):\n",
    "                temp .append(input_tensor[j])\n",
    "                \n",
    "        temp = torch.stack(temp)\n",
    "        temp_list.append(temp)\n",
    "    return torch.stack(temp_list), S\n",
    "\n",
    "GeneratorNet = nn.Sequential(                      # Generator\n",
    "    # random ideas (could from normal distribution)\n",
    "    nn.Linear(N_IDEAS, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    # making a painting from these random ideas\n",
    "    nn.Linear(64, ART_COMPONENTS),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T05:50:03.838759Z",
     "start_time": "2021-06-14T05:50:02.143700Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "random.seed(2000)\n",
    "torch.manual_seed(256)\n",
    "DiscriminatorNet  = Net()\n",
    "DiscriminatorNet.apply(weight_init)\n",
    "GeneratorNet.apply(weight_init)\n",
    "# DiscriminatorNet = torch.load(\"save/Deep_learning_D_4_gan\")\n",
    "# if(Is_GAN):\n",
    "#     GeneratorNet = torch.load(\"save/Deep_learning_G_4)\n",
    "DiscriminatorNet.to(dev)\n",
    "GeneratorNet.to(dev)\n",
    "\n",
    "opt_D = torch.optim.Adam(DiscriminatorNet.parameters(), lr=LR_D)\n",
    "opt_G = torch.optim.Adam(GeneratorNet.parameters(), lr=LR_G)\n",
    "\n",
    "\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(opt_D, step_size=100, gamma=0.98)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(opt_G, step_size=100, gamma=0.98)\n",
    "\n",
    "index_train_list = []\n",
    "index_test_list = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T06:00:48.103538Z",
     "start_time": "2021-06-14T05:50:03.838759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(11.8895, device='cuda:0', grad_fn=<DivBackward0>) 760.9251708984375\n",
      "100 tensor(0.0166, device='cuda:0', grad_fn=<DivBackward0>) 1.0617752075195312\n",
      "200 tensor(0.0027, device='cuda:0', grad_fn=<DivBackward0>) 0.17275437712669373\n",
      "300 tensor(0.0013, device='cuda:0', grad_fn=<DivBackward0>) 0.08537135273218155\n",
      "400 tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>) 0.04573811963200569\n",
      "500 tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>) 0.01555523183196783\n",
      "600 tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>) 0.016529617831110954\n",
      "700 tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>) 0.01156578678637743\n",
      "800 tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>) 0.006742805242538452\n",
      "900 tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>) 0.007298856973648071\n",
      "1000 tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>) 0.009576770476996899\n",
      "1100 tensor(7.9314e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.005076082888990641\n",
      "1200 tensor(8.6469e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.005534040741622448\n",
      "1300 tensor(4.8236e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.003087124088779092\n",
      "1400 tensor(4.2258e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0027044962625950575\n",
      "1500 tensor(5.1039e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0032664705067873\n",
      "1600 tensor(5.3868e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0034475724678486586\n",
      "1700 tensor(3.7772e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.002417387906461954\n",
      "1800 tensor(3.4909e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0022341578733175993\n",
      "1900 tensor(3.1627e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0020241541787981987\n",
      "2000 tensor(4.5499e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0029119604732841253\n"
     ]
    }
   ],
   "source": [
    "#supervised\n",
    "index_supervisedtrain_list = []\n",
    "supervisedtrain_losses = []\n",
    "for iteration in range(int(echo)):\n",
    "\n",
    "    temp_number = 0\n",
    "    total_batch_loss = 0 \n",
    "    \n",
    "    loss_sum = 0\n",
    "    denominator = 0\n",
    "    for index in range(0, BATCH_SIZE):\n",
    "        training_data_i, training_S, training_data,label = appen_train(\n",
    "            Generate_distribution(Agent_number_n))\n",
    "        h_loss = DiscriminatorNet.supervised_loss(training_data_i, label)\n",
    "        denominator += 1\n",
    "        loss_sum += h_loss\n",
    "\n",
    "    loss_sum = torch.sum(loss_sum)\n",
    "    loss = (loss_sum) / denominator \n",
    "    total_batch_loss += float(loss_sum)\n",
    "\n",
    "    opt_D.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_D.step()\n",
    "\n",
    "    if (iteration%100 == 0):\n",
    "        print(iteration,loss,total_batch_loss)\n",
    "        index_supervisedtrain_list.append(iteration)\n",
    "        supervisedtrain_losses.append(total_batch_loss)\n",
    "    scheduler_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T06:00:48.229412Z",
     "start_time": "2021-06-14T06:00:48.104732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ylim(0, 0.1)\n",
    "plt.plot(index_supervisedtrain_list ,supervisedtrain_losses)\n",
    "plt.ylabel('supervised train loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3133, device='cuda:0', grad_fn=<MaxBackward1>) tensor([3.8614e-07], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.48879873752594 3.056211860677314 0.43258687684862585 0.11658501763094176\n",
      "\n",
      "\n",
      "100 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.2903, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.6998e-07], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.5769824981689453 3.049916959125821 0.5270655390431243 0.12682942067657166\n",
      "\n",
      "\n",
      "200 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3397, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.1429e-08], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.377850665827567 3.004222518691372 0.37362814713619485 0.12586662326978448\n",
      "\n",
      "\n",
      "300 tensor(3.6713e-08, device='cuda:0', grad_fn=<DivBackward0>) 0.0 2.3496006633649813e-06\n",
      "Gan: tensor(-0.3339, device='cuda:0', grad_fn=<MaxBackward1>) tensor([9.5056e-09], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.371721659135913 3.0010256438609786 0.37069601527493434 0.12777426313535667\n",
      "\n",
      "\n",
      "400 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3272, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.9754e-08], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.401009098082999 3.015609144449099 0.3853999536338999 0.12003735669803106\n",
      "\n",
      "\n",
      "500 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3374, device='cuda:0', grad_fn=<MaxBackward1>) tensor([3.4495e-09], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.3817435669890665 3.004616409283898 0.3771271577051687 0.12391942380804011\n",
      "\n",
      "\n",
      "600 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.2908, device='cuda:0', grad_fn=<MaxBackward1>) tensor([4.7070e-08], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.4326330423355103 3.031455685283612 0.4011773570518984 0.11823793361847379\n",
      "\n",
      "\n",
      "700 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3044, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.6999e-09], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.3578708171844482 3.006698966837213 0.3511718503472352 0.12248390539173037\n",
      "\n",
      "\n",
      "800 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.2986, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.5283e-08], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.430509775942715 3.032433579565445 0.39807619637726965 0.10955464200596943\n",
      "\n",
      "\n",
      "900 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3145, device='cuda:0', grad_fn=<MaxBackward1>) tensor([3.0708e-10], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.3957732294509175 3.0011448860168457 0.3946283434340718 0.12957900198092176\n",
      "\n",
      "\n",
      "1000 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3184, device='cuda:0', grad_fn=<MaxBackward1>) tensor([8.9681e-11], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.3911265849508205 2.9948819875717163 0.3962445973791042 0.13927199302302107\n",
      "\n",
      "\n",
      "1100 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3077, device='cuda:0', grad_fn=<MaxBackward1>) tensor([5.6188e-11], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.395335015486736 3.0126218795776367 0.3827131359090994 0.11735774681274158\n",
      "\n",
      "\n",
      "1200 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3030, device='cuda:0', grad_fn=<MaxBackward1>) tensor([5.2394e-11], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.4240322220572437 3.025609351561544 0.3984228704956996 0.10998638560980822\n",
      "\n",
      "\n",
      "1300 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3151, device='cuda:0', grad_fn=<MaxBackward1>) tensor([0.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.388975697584297 3.0120857315766205 0.3768899660076763 0.11384490587439533\n",
      "\n",
      "\n",
      "1400 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3152, device='cuda:0', grad_fn=<MaxBackward1>) tensor([0.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.3805177057995714 3.008848039661892 0.3716696661376795 0.11485372218788825\n",
      "\n",
      "\n",
      "1500 tensor(5.9405e-09, device='cuda:0', grad_fn=<DivBackward0>) 0.0 3.801932564329036e-07\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(int(echo)):\n",
    "\n",
    "    temp_number = 0\n",
    "    total_batch_loss = 0 \n",
    "        \n",
    "    loss2_list = []\n",
    "    loss1_sum = 0\n",
    "    loss2_sum = 0\n",
    "    denominator = 0\n",
    "    for index in range(0, BATCH_SIZE):\n",
    "        training_data_i, training_label, training_data, label = appen_train(\n",
    "            Generate_distribution(Agent_number_n))\n",
    "        h_loss1, h_loss2 = DiscriminatorNet(training_data_i, training_label,\n",
    "                                       training_data)\n",
    "        denominator += 1\n",
    "        loss1_sum += h_loss1\n",
    "        loss2_sum += h_loss2\n",
    "\n",
    "    loss_sum = torch.sum(loss1_sum + loss2_sum)\n",
    "    loss = (loss_sum) / denominator \n",
    "    total_batch_loss +=float(loss_sum)\n",
    "\n",
    "    opt_D.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_D.step()\n",
    "\n",
    "    temp_number = iteration\n",
    "    index_train_list.append(iteration)\n",
    "    train_losses.append(total_batch_loss)\n",
    "\n",
    "    if (iteration%100 == 0):\n",
    "        print(temp_number,loss,float(loss1_sum),float(loss2_sum))\n",
    "    ## Gan \n",
    "        if(Is_GAN):## Gan Work traning GeneratorNet\n",
    "\n",
    "            DiscriminatorNet.requires_grad = True\n",
    "            GeneratorNet.requires_grad = True\n",
    "\n",
    "\n",
    "            for step in range(100):\n",
    "                # real painting from artist\n",
    "                G_ideas = torch.randn(BATCH_SIZE, N_IDEAS,\n",
    "                                      requires_grad=True).to(dev)  # random ideas\\n\n",
    "                # fake painting from G (random ideas)\n",
    "\n",
    "                G_values = GeneratorNet(G_ideas)\n",
    "                G_values , indices = torch.sort(G_values, descending=True)\n",
    "            #     print(artist_paintings)\n",
    "            #     print(G_paintings)\n",
    "\n",
    "                result_list = []\n",
    "                for index in range(BATCH_SIZE):\n",
    "                    h_list = []\n",
    "                    value_list_tensor, S_tensor = redistribution_value_function(\n",
    "                        G_values[index])\n",
    "                    for i in range(Agent_number_n):\n",
    "                        h = DiscriminatorNet.calculate(\n",
    "                            value_list_tensor[i].cuda().type(torch.float32))\n",
    "                        h_list.append(h)\n",
    "                    h_list = torch.stack(h_list)\n",
    "                    result_list.append(torch.sum(h_list)/S_tensor.cuda())\n",
    "                result_list = torch.stack(result_list)\n",
    "\n",
    "                diff_loss = torch.max(result_list)-torch.min(result_list)\n",
    "                G_loss = torch.max(- diff_loss)\n",
    "\n",
    "                opt_G.zero_grad()\n",
    "                G_loss.backward()\n",
    "                opt_G.step()\n",
    "\n",
    "            # real painting from artist\n",
    "                G_ideas = torch.randn(BATCH_SIZE, N_IDEAS,\n",
    "                                      requires_grad=True).to(dev)  # random ideas\\n\n",
    "                # fake painting from G (random ideas)\n",
    "\n",
    "                G_values = GeneratorNet(G_ideas)\n",
    "                G_values , indices = torch.sort(G_values, descending=True)\n",
    "            #     print(artist_paintings)\n",
    "            #     print(G_paintings)\n",
    "\n",
    "                result_list = []\n",
    "                for index in range(BATCH_SIZE):\n",
    "                    h_list = []\n",
    "                    value_list_tensor, S_tensor = redistribution_value_function(\n",
    "                        G_values[index])\n",
    "                    for i in range(Agent_number_n):\n",
    "                        h = DiscriminatorNet.calculate(\n",
    "                            value_list_tensor[i].cuda().type(torch.float32))\n",
    "                        h_list.append(h)\n",
    "                    h_list = torch.stack(h_list)\n",
    "                    result_list.append(torch.sum(h_list)/S_tensor.cuda())\n",
    "                result_list = torch.stack(result_list)\n",
    "\n",
    "                diff_loss = torch.max(result_list)-torch.min(result_list)\n",
    "\n",
    "                D_loss = torch.where((Agent_number_n-1)>torch.min(result_list),\n",
    "                    torch.square(((Agent_number_n-1)-torch.min(result_list))),\n",
    "                    torch.zeros(1).to(dev)\n",
    "                  )   + torch.where((Agent_number_n-Alpha)<torch.max(result_list),\n",
    "                                torch.square((torch.max(result_list)-(Agent_number_n-Alpha)))/10000,\n",
    "                                torch.zeros(1).to(dev)\n",
    "                              )\n",
    "\n",
    "\n",
    "                opt_D.zero_grad()\n",
    "                D_loss.backward()\n",
    "                opt_D.step()\n",
    "                \n",
    "            print(\"Gan:\",G_loss,D_loss)\n",
    "            print()\n",
    "\n",
    "            \n",
    "        result_list = []\n",
    "        for index in range(len(testing_data)):\n",
    "            h_list = []\n",
    "            for i in range(Agent_number_n):\n",
    "                h = DiscriminatorNet.calculate(\n",
    "                    torch.tensor(testing_data[index][i]).to(dev).type(\n",
    "                        torch.float32))\n",
    "                h_list.append(float(h))\n",
    "            result_list.append(sum(h_list) / testing_label[index])\n",
    "        print(max(result_list), min(result_list),\n",
    "              max(result_list) - min(result_list),\n",
    "              (sum(result_list) / len(result_list) - min(result_list))+ Agent_number_n-1 - min(min(result_list),Agent_number_n-1) )\n",
    "\n",
    "        index_test_list.append(iteration)\n",
    "        test_losses.append(\n",
    "            (sum(result_list) / len(result_list) - min(result_list))+ Agent_number_n-1 - min(min(result_list),Agent_number_n-1) )\n",
    "        print()\n",
    "        index_test_list.append(iteration)\n",
    "        test_losses.append(\n",
    "            max(result_list)-min(result_list) )\n",
    "        \n",
    "  \n",
    "        print()\n",
    "\n",
    "\n",
    "    scheduler_D.step()\n",
    "    scheduler_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.550Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylim(0, 1.0)\n",
    "plt.plot(index_test_list,test_losses)\n",
    "plt.ylabel('test loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.551Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(DiscriminatorNet, \"save/Deep_learning_D_10_gan\")\n",
    "if(Is_GAN):\n",
    "    torch.save(GeneratorNet, \"save/Deep_learning_G_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.553Z"
    }
   },
   "outputs": [],
   "source": [
    "def h_3_star(a, b, t):\n",
    "    return a - min(a, t) + b - min(b, t) + max(min(a, t)+min(b, t), 2*t/3) + 1/2 * max(min(a, t)+min(b, t), t) - 1/2 * max(max(min(a, t), min(b, t)), 2*t/3) - t/6\n",
    "\n",
    "\n",
    "def f_function(a, b, z):\n",
    "    if(z >= 1):\n",
    "        return (a+b)/2 + z/3\n",
    "    else:\n",
    "        return z/3 + h_3_star(a, b, 1-z)/2\n",
    "\n",
    "def h_function(input_list):\n",
    "    #input_list = sorted(input_list)\n",
    "    g_list = []\n",
    "    for j1 in range(len(input_list) ):\n",
    "        for j2 in range(len(input_list)):\n",
    "            if(j1 != j2):\n",
    "                a = input_list[j1]\n",
    "                b = input_list[j2]\n",
    "                z = sum(input_list)- a-b\n",
    "\n",
    "                g_list.append( f_function(a, b, z) * (Agent_number_n-1))\n",
    "    h = sum(g_list) * 3 /  (Agent_number_n) /  (Agent_number_n-1) /  (Agent_number_n - 2)\n",
    "    return h\n",
    "                \n",
    "                \n",
    "x_list = []\n",
    "y_list = []\n",
    "z_list = []\n",
    "result_list = []\n",
    "for index in range(len(testing_data)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        x_list.append(testing_data[index][i][0])\n",
    "        y_list.append(testing_data[index][i][1])\n",
    "        h = h_function(testing_data[index][i])\n",
    "        z_list.append(float(h))\n",
    "        h_list.append(float(h))\n",
    "    result_list.append(sum(h_list)/testing_label[index]) \n",
    "    \n",
    "    \n",
    "print(max(result_list), min(result_list), max(result_list)-min(result_list),sum(result_list)/len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.554Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list)/len(result_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.556Z"
    }
   },
   "outputs": [],
   "source": [
    "denominator = 0\n",
    "result_list = []\n",
    "for index in range(len(testing_data)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        h = DiscriminatorNet.calculate(torch.tensor(testing_data[index][i]).to(dev).type(torch.float32))\n",
    "        h_list.append(float(h))\n",
    "    \n",
    "    #print(sum(h_list),h_list,sum(h_list)/testing_label[index])\n",
    "    result_list.append(sum(h_list)/testing_label[index])\n",
    "    \n",
    "\n",
    "print(max(result_list),min(result_list),max(result_list)-min(result_list))\n",
    "print(sum(result_list)/len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.557Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list)/len(result_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.558Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate 10000 testing data on GeneratorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.559Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def appen_test_G(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    testing_data_G.append(temp_list)\n",
    "    testing_label_G.append(S)\n",
    "# fake painting from G (random ideas)\n",
    "\n",
    "def read_testing_data_G():\n",
    "    for i in range(10000):\n",
    "        #appen_test_G(sorted(np.random.rand(Agent_number_n), reverse=True));\n",
    "        G_ideas = torch.randn(N_IDEAS).to(dev)  # random ideas\\n\n",
    "        G_values = GeneratorNet(G_ideas)\n",
    "        G_values , indices = torch.sort(G_values, descending=True)\n",
    "        appen_test_G(G_values.detach().cpu().numpy()) \n",
    "        \n",
    "testing_data_G = []\n",
    "testing_label_G = []\n",
    "read_testing_data_G()\n",
    "testing_data_G=np.array(testing_data_G)\n",
    "testing_label_G=np.array(testing_label_G)\n",
    "print(testing_data_G)\n",
    "print(testing_label_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.560Z"
    }
   },
   "outputs": [],
   "source": [
    "result_list_G = []\n",
    "for index in range(len(testing_data_G)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        h = DiscriminatorNet.calculate(torch.tensor(testing_data_G[index][i]).to(dev).type(torch.float32))\n",
    "        h_list.append(float(h))\n",
    "    \n",
    "    #print(sum(h_list),h_list,sum(h_list)/testing_label[index])\n",
    "    result_list_G.append(sum(h_list)/testing_label_G[index])\n",
    "    \n",
    "\n",
    "print(max(result_list_G),min(result_list_G),max(result_list_G)-min(result_list_G))\n",
    "print(sum(result_list_G)/len(result_list_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.561Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list_G,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list_G)/len(result_list_G), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list_G.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list_G)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list_G)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-14T05:49:59.563Z"
    }
   },
   "outputs": [],
   "source": [
    "final_list = result_list + result_list_G\n",
    "\n",
    "\n",
    "plt.hist(final_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(final_list)/len(final_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
