{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T15:59:25.053899Z",
     "start_time": "2021-06-12T15:59:22.999045Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pygame\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as opt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy.stats as st\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from matplotlib.colors import LogNorm \n",
    "import matplotlib.cm as cm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy.interpolate import griddata\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "\n",
    "print(dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T15:59:25.069364Z",
     "start_time": "2021-06-12T15:59:25.055894Z"
    }
   },
   "outputs": [],
   "source": [
    "global temp_list\n",
    "temp_list = []\n",
    "Agent_number_n=3;\n",
    "Alpha = 0.66 \n",
    "\n",
    "# Hyper Parameters\n",
    "echo = 5001 \n",
    "BATCH_SIZE = 64\n",
    "LR_G = 0.001           # learning rate for generator\n",
    "LR_D = 0.001           # learning rate for discriminator\n",
    "N_IDEAS = Agent_number_n             # think of this as number of ideas for generating an art work (Generator)\n",
    "ART_COMPONENTS = Agent_number_n     # it could be total point G can draw in the canvas\n",
    "\n",
    "Is_GAN = True # if use Gan\n",
    "\n",
    "def Generate_distribution(Agent_number_n):\n",
    "    return sorted(np.random.rand(Agent_number_n), reverse=True)\n",
    "    #return sorted(np.random.normal(normalloc,normalscale,Agent_number_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T15:59:25.336161Z",
     "start_time": "2021-06-12T15:59:25.070879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.31193301 0.06727375]\n",
      "  [0.81134375 0.06727375]\n",
      "  [0.81134375 0.31193301]]\n",
      "\n",
      " [[0.28656632 0.15872574]\n",
      "  [0.40800421 0.15872574]\n",
      "  [0.40800421 0.28656632]]\n",
      "\n",
      " [[0.60090867 0.37456457]\n",
      "  [0.71440292 0.37456457]\n",
      "  [0.71440292 0.60090867]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.4398485  0.30260382]\n",
      "  [0.86124169 0.30260382]\n",
      "  [0.86124169 0.4398485 ]]\n",
      "\n",
      " [[0.36658574 0.21544923]\n",
      "  [0.98443368 0.21544923]\n",
      "  [0.98443368 0.36658574]]\n",
      "\n",
      " [[0.07775046 0.04065142]\n",
      "  [0.53020164 0.04065142]\n",
      "  [0.53020164 0.07775046]]]\n",
      "[1.19055051 1.         1.68987616 ... 1.603694   1.56646866 1.        ]\n"
     ]
    }
   ],
   "source": [
    "def appen(_x_list,y):\n",
    "    global temp_list\n",
    "    temp_list.append(_x_list)\n",
    "    \n",
    "def appen_train(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    \n",
    "    temp_list = np.array(temp_list)\n",
    "    x_list = np.array(x_list)\n",
    "    return temp_list,S,x_list\n",
    "    \n",
    "\n",
    "def appen_test(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    testing_data.append(temp_list)\n",
    "    testing_label.append(S)\n",
    "    temp_list = np.array(temp_list)\n",
    "    return temp_list,S\n",
    "    \n",
    "\n",
    "def read_testing_data():\n",
    "    for i in range(10000):\n",
    "        appen_test(Generate_distribution(Agent_number_n));\n",
    "                            \n",
    "\n",
    "testing_data=[]\n",
    "testing_label=[]\n",
    "S=1.0\n",
    "read_testing_data();\n",
    "\n",
    "testing_data=np.array(testing_data)\n",
    "testing_label=np.array(testing_label)\n",
    "print(testing_data)\n",
    "print(testing_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T15:59:26.191621Z",
     "start_time": "2021-06-12T15:59:25.338156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3332878853008667 2.0 0.3332878853008667 2.1026065389064077\n"
     ]
    }
   ],
   "source": [
    "def h_3_star(a, b, t):\n",
    "    return a - min(a, t) + b - min(b, t) + max(min(a, t)+min(b, t), 2*t/3) + 1/2 * max(min(a, t)+min(b, t), t) - 1/2 * max(max(min(a, t), min(b, t)), 2*t/3) - t/6\n",
    "\n",
    "\n",
    "def f_function(a, b, z):\n",
    "    if(z >= 1):\n",
    "        return (a+b)/2 + z/3\n",
    "    else:\n",
    "        return z/3 + h_3_star(a, b, 1-z)/2\n",
    "\n",
    "def h_function(input_list):\n",
    "    #input_list = sorted(input_list)\n",
    "    g_list = []\n",
    "    for j1 in range(len(input_list) ):\n",
    "        for j2 in range(len(input_list)):\n",
    "            if(j1 != j2):\n",
    "                a = input_list[j1]\n",
    "                b = input_list[j2]\n",
    "                z = sum(input_list)- a-b\n",
    "\n",
    "                g_list.append( f_function(a, b, z) * (Agent_number_n-1))\n",
    "    h = sum(g_list) * 3 /  (Agent_number_n) /  (Agent_number_n-1) /  (Agent_number_n - 2)\n",
    "    return h\n",
    "                \n",
    "                \n",
    "x_list = []\n",
    "y_list = []\n",
    "z_list = []\n",
    "result_list = []\n",
    "for index in range(len(testing_data)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        x_list.append(testing_data[index][i][0])\n",
    "        y_list.append(testing_data[index][i][1])\n",
    "        h = h_function(testing_data[index][i])\n",
    "        z_list.append(float(h))\n",
    "        h_list.append(float(h))\n",
    "    result_list.append(sum(h_list)/testing_label[index]) \n",
    "    \n",
    "    \n",
    "print(max(result_list), min(result_list), max(result_list)-min(result_list),sum(result_list)/len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T15:59:27.482900Z",
     "start_time": "2021-06-12T15:59:26.193619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(result_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list)/len(result_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T15:59:27.513831Z",
     "start_time": "2021-06-12T15:59:27.484894Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.input_dim = (Agent_number_n-1)\n",
    "        self.hidden_dim = 128\n",
    "        self.output_dim = 1\n",
    "        self.hidden_layer_count = 6 \n",
    "        \n",
    "        current_dim = self.input_dim\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(self.hidden_layer_count):\n",
    "            self.layers.append(torch.nn.Linear(current_dim, self.hidden_dim))\n",
    "            current_dim = self.hidden_dim\n",
    "        self.layers.append(torch.nn.Linear(current_dim, self.output_dim))\n",
    "\n",
    "    def calculate(self, value_list):\n",
    "        h = value_list\n",
    "#         LeakyReLU = torch.nn.LeakyReLU()\n",
    "        for layer in self.layers:\n",
    "            h = torch.relu(layer(h))\n",
    "        out = F.softmax(self.layers[-1](x))\n",
    "        return out    \n",
    "        return h\n",
    "\n",
    "    def forward(self, input_list,input_label,label):\n",
    "        global iteration,echo,target_order\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        loss3 = 0\n",
    "        input_list = torch.from_numpy(\n",
    "            np.array(input_list)).to(dev).type(torch.float32)\n",
    "        h_list = []\n",
    "\n",
    "        for i in range(Agent_number_n):\n",
    "            h = self.calculate(input_list[i])\n",
    "            h_list.append(h)\n",
    "#             loss3 += torch.square(h_function_2(input_list)-h2[1])\n",
    "            \n",
    "        input_label = torch.from_numpy(\n",
    "            np.array(input_label)).to(dev).type(torch.float32)\n",
    "        sum_h = torch.sum(torch.cat(h_list)).to(dev)\n",
    "\n",
    "\n",
    "        loss1 = torch.where((Agent_number_n-1)*input_label>sum_h,\n",
    "                        torch.square(((Agent_number_n-1)*input_label-sum_h)),\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "        loss2 = torch.where((Agent_number_n-Alpha)*input_label<sum_h,\n",
    "                        torch.square((sum_h-(Agent_number_n-Alpha)*input_label))/100,\n",
    "                        torch.zeros(1).to(dev)\n",
    "                      )\n",
    "\n",
    "\n",
    "        return loss1,loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T15:59:27.528791Z",
     "start_time": "2021-06-12T15:59:27.515826Z"
    }
   },
   "outputs": [],
   "source": [
    "def redistribution_value_function(input_tensor):\n",
    "    S = torch.max(torch.sum(input_tensor), torch.ones(1).to(dev))\n",
    "    temp_list = []\n",
    "\n",
    "\n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        \n",
    "        for j in range(Agent_number_n):\n",
    "            if(i != j):\n",
    "                temp .append(input_tensor[j])\n",
    "                \n",
    "        temp = torch.stack(temp)\n",
    "        temp_list.append(temp)\n",
    "    return torch.stack(temp_list), S\n",
    "\n",
    "GeneratorNet = nn.Sequential(                      # Generator\n",
    "    # random ideas (could from normal distribution)\n",
    "    nn.Linear(N_IDEAS, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    # making a painting from these random ideas\n",
    "    nn.Linear(64, ART_COMPONENTS),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T15:59:29.004987Z",
     "start_time": "2021-06-12T15:59:27.530787Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "random.seed(2000)\n",
    "torch.manual_seed(256)\n",
    "DiscriminatorNet  = Net()\n",
    "DiscriminatorNet.apply(weight_init)\n",
    "GeneratorNet.apply(weight_init)\n",
    "# DiscriminatorNet = torch.load(\"save/Deep_learning_D_3_1\")\n",
    "# GeneratorNet = torch.load(\"save/Deep_learning_G_3_1\")\n",
    "DiscriminatorNet.to(dev)\n",
    "GeneratorNet.to(dev)\n",
    "\n",
    "opt_D = torch.optim.Adam(DiscriminatorNet.parameters(), lr=LR_D)\n",
    "opt_G = torch.optim.Adam(GeneratorNet.parameters(), lr=LR_G)\n",
    "\n",
    "\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(opt_D, step_size=100, gamma=0.98)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(opt_G, step_size=100, gamma=0.98)\n",
    "\n",
    "index_train_list = []\n",
    "index_test_list = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.434477Z",
     "start_time": "2021-06-12T15:59:29.007979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(10.8084, device='cuda:0', grad_fn=<DivBackward0>) 691.7366943359375 0.0\n",
      "Gan: tensor(-1.3579, device='cuda:0', grad_fn=<MaxBackward1>) tensor([0.0015], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "6.259364366531372 2.413051128387451 3.846313238143921 2.7798867224722086\n",
      "\n",
      "\n",
      "100 tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.01815234124660492\n",
      "Gan: tensor(-0.7707, device='cuda:0', grad_fn=<MaxBackward1>) tensor([2.0326e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.808471381664276 1.6027927994728088 1.2056785821914673 1.0551479879666896\n",
      "\n",
      "\n",
      "200 tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>) 0.004053208045661449 0.005363836418837309\n",
      "Gan: tensor(-0.8381, device='cuda:0', grad_fn=<MaxBackward1>) tensor([3.4135e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.9150211811065674 1.900099515914917 1.0149216651916504 0.5342273626771794\n",
      "\n",
      "\n",
      "300 tensor(9.0970e-05, device='cuda:0', grad_fn=<DivBackward0>) 4.316336344345473e-06 0.005817738361656666\n",
      "Gan: tensor(-0.7725, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.9023e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.7607033252716064 1.8964630961418152 0.8642402291297913 0.41726742588231636\n",
      "\n",
      "\n",
      "400 tensor(6.9896e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.004473349079489708\n",
      "Gan: tensor(-0.8083, device='cuda:0', grad_fn=<MaxBackward1>) tensor([2.2579e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.7986543774604797 1.9848244190216064 0.8138299584388733 0.2916873499508683\n",
      "\n",
      "\n",
      "500 tensor(5.2387e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.003352789906784892\n",
      "Gan: tensor(-0.8119, device='cuda:0', grad_fn=<MaxBackward1>) tensor([5.5731e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.0842748880386353 2.1921533346176147 0.8921215534210205 0.3418528013238409\n",
      "\n",
      "\n",
      "600 tensor(6.0636e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0038806803058832884\n",
      "Gan: tensor(-0.7193, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.4314e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.7532856464385986 2.0095870448413016 0.7436986015972971 0.2740942794578012\n",
      "\n",
      "\n",
      "700 tensor(1.9608e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0012549186358228326\n",
      "Gan: tensor(-0.7413, device='cuda:0', grad_fn=<MaxBackward1>) tensor([5.9036e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.1273418802848876 2.2921460270881653 0.8351958531967223 0.30232716226575373\n",
      "\n",
      "\n",
      "800 tensor(2.9789e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0019065268570557237\n",
      "Gan: tensor(-0.6214, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.0555e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.6995591690217786 2.0236722230911255 0.6758869459306531 0.21099845381183435\n",
      "\n",
      "\n",
      "900 tensor(1.1264e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0007209152681753039\n",
      "Gan: tensor(-0.5590, device='cuda:0', grad_fn=<MaxBackward1>) tensor([5.8050e-06], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.6440117457837986 2.0056722358031767 0.6383395099806219 0.17946436455270387\n",
      "\n",
      "\n",
      "1000 tensor(1.8738e-05, device='cuda:0', grad_fn=<DivBackward0>) 5.0561233365442604e-05 0.0011486868606880307\n",
      "Gan: tensor(-0.6190, device='cuda:0', grad_fn=<MaxBackward1>) tensor([2.4716e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.932861513970891 2.1778008937835693 0.7550606201873218 0.24277563512058098\n",
      "\n",
      "\n",
      "1100 tensor(1.1224e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0007183669367805123\n",
      "Gan: tensor(-0.5750, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.3118e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.8117281859326853 2.0889695286750793 0.7227586572576059 0.24477527605591876\n",
      "\n",
      "\n",
      "1200 tensor(8.0924e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0005179146537557244\n",
      "Gan: tensor(-0.4884, device='cuda:0', grad_fn=<MaxBackward1>) tensor([9.1938e-07], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.6478545593751512 2.0041101447777705 0.6437444145973807 0.18729981426165354\n",
      "\n",
      "\n",
      "1300 tensor(1.3412e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0008583497256040573\n",
      "Gan: tensor(-0.4737, device='cuda:0', grad_fn=<MaxBackward1>) tensor([2.2442e-06], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.6736898115203838 2.0090335152950307 0.664656296225353 0.1827474592572078\n",
      "\n",
      "\n",
      "1400 tensor(1.0416e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.0 6.666063563898206e-05\n",
      "Gan: tensor(-0.4824, device='cuda:0', grad_fn=<MaxBackward1>) tensor([6.2817e-06], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.754670814778501 2.0883452892303467 0.6663255255481544 0.18001400005612034\n",
      "\n",
      "\n",
      "1500 tensor(1.6247e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.00010398012091172859\n",
      "Gan: tensor(-0.4162, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.7448e-06], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.6296562972090527 2.0201569199562073 0.6094993772528454 0.15204824893032987\n",
      "\n",
      "\n",
      "1600 tensor(2.6017e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.00016650804900564253\n",
      "Gan: tensor(-0.5249, device='cuda:0', grad_fn=<MaxBackward1>) tensor([7.6708e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "3.5616024146132608 2.680151704055075 0.8814507105581857 0.1922646350615529\n",
      "\n",
      "\n",
      "1700 tensor(5.9392e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0003801065613515675\n",
      "Gan: tensor(-0.4293, device='cuda:0', grad_fn=<MaxBackward1>) tensor([2.8945e-06], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.6501437701944117 2.043787360191345 0.6063564100030665 0.17199941768543825\n",
      "\n",
      "\n",
      "1800 tensor(8.8299e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0005651149549521506\n",
      "Gan: tensor(-0.5291, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.2307e-05], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.89853435754776 2.086482033709367 0.812052323838393 0.21127655740150608\n",
      "\n",
      "\n",
      "1900 tensor(1.1624e-05, device='cuda:0', grad_fn=<DivBackward0>) 0.0007049940177239478 3.892654422088526e-05\n",
      "Gan: tensor(-0.4278, device='cuda:0', grad_fn=<MaxBackward1>) tensor([2.1314e-06], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.592568650076856 2.0206730365753174 0.5718956135015385 0.16570433561668807\n",
      "\n",
      "\n",
      "2000 tensor(5.0647e-07, device='cuda:0', grad_fn=<DivBackward0>) 0.0 3.241412196075544e-05\n",
      "Gan: tensor(-0.3990, device='cuda:0', grad_fn=<MaxBackward1>) tensor([4.3345e-07], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.547977587334354 1.9810915408567515 0.5668860464776024 0.17457088542747012\n",
      "\n",
      "\n",
      "2100 tensor(5.7720e-07, device='cuda:0', grad_fn=<DivBackward0>) 0.0 3.694049519253895e-05\n",
      "Gan: tensor(-0.3627, device='cuda:0', grad_fn=<MaxBackward1>) tensor([5.3455e-07], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.501470100143859 1.9810185432434082 0.5204515569004506 0.1873285380810663\n",
      "\n",
      "\n",
      "2200 tensor(7.1020e-07, device='cuda:0', grad_fn=<DivBackward0>) 0.0 4.545291812974028e-05\n",
      "Gan: tensor(-0.3664, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.5466e-07], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.488179924954627 1.9882295727729797 0.4999503521816471 0.1701483759262552\n",
      "\n",
      "\n",
      "2300 tensor(4.6721e-08, device='cuda:0', grad_fn=<DivBackward0>) 0.0 2.9901402740506455e-06\n",
      "Gan: tensor(-0.3561, device='cuda:0', grad_fn=<MaxBackward1>) tensor([5.0614e-08], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.473465925287708 1.9751092692296273 0.49835665605808055 0.17848882610536432\n",
      "\n",
      "\n",
      "2400 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3373, device='cuda:0', grad_fn=<MaxBackward1>) tensor([6.5528e-08], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.4587659236452306 1.97829412939753 0.4804717942477006 0.17477280341085466\n",
      "\n",
      "\n",
      "2500 tensor(5.4585e-07, device='cuda:0', grad_fn=<DivBackward0>) 0.0 3.49343717971351e-05\n",
      "Gan: tensor(-0.3417, device='cuda:0', grad_fn=<MaxBackward1>) tensor([5.6957e-10], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.451053368059574 1.9751731941106436 0.47588017394893023 0.1804184334105996\n",
      "\n",
      "\n",
      "2600 tensor(5.0751e-07, device='cuda:0', grad_fn=<DivBackward0>) 0.0 3.2480696972925216e-05\n",
      "Gan: tensor(-0.3449, device='cuda:0', grad_fn=<MaxBackward1>) tensor([3.6760e-09], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.438066048433061 1.9813337922096252 0.45673225622343594 0.17507819470959696\n",
      "\n",
      "\n",
      "2700 tensor(6.3819e-07, device='cuda:0', grad_fn=<DivBackward0>) 0.0 4.0843933675205335e-05\n",
      "Gan: tensor(-0.3308, device='cuda:0', grad_fn=<MaxBackward1>) tensor([0.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.4295557833951893 1.9745187303031138 0.45503705309207554 0.1804661579218667\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800 tensor(4.6066e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.00029471860034391284 1.0689198148838841e-07\n",
      "Gan: tensor(-0.3515, device='cuda:0', grad_fn=<MaxBackward1>) tensor([4.8620e-08], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.467406144918496 1.9942636489868164 0.47314249593167945 0.16375086311914222\n",
      "\n",
      "\n",
      "2900 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.4274, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.7772e-07], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.5087936767282795 2.034315335133534 0.47447834159474533 0.1521503096215091\n",
      "\n",
      "\n",
      "3000 tensor(2.3113e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.00014699286839459091 9.326579402113566e-07\n",
      "Gan: tensor(-0.3196, device='cuda:0', grad_fn=<MaxBackward1>) tensor([3.1007e-07], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.4381702625387596 1.9883375763893127 0.4498326861494468 0.17550244395872827\n",
      "\n",
      "\n",
      "3100 tensor(0., device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.0\n",
      "Gan: tensor(-0.3206, device='cuda:0', grad_fn=<MaxBackward1>) tensor([0.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.4376366175137334 1.9935155303356087 0.4441210871781247 0.15358589439638326\n",
      "\n",
      "\n",
      "3200 tensor(1.7680e-06, device='cuda:0', grad_fn=<DivBackward0>) 0.0 0.00011315107985865325\n",
      "Gan: tensor(-0.3165, device='cuda:0', grad_fn=<MaxBackward1>) tensor([0.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.4065357356317305 1.9727429151535034 0.4337928204782271 0.18063821723073437\n",
      "\n",
      "\n",
      "3300 tensor(2.2260e-09, device='cuda:0', grad_fn=<DivBackward0>) 0.0 1.4246128898776078e-07\n",
      "Gan: tensor(-0.3106, device='cuda:0', grad_fn=<MaxBackward1>) tensor([0.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.413648570771901 1.9856167755994925 0.42803179517240864 0.16170065658698052\n",
      "\n",
      "\n",
      "3400 tensor(1.3104e-07, device='cuda:0', grad_fn=<DivBackward0>) 0.0 8.386632543988526e-06\n",
      "Gan: tensor(-0.3144, device='cuda:0', grad_fn=<MaxBackward1>) tensor([0.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.398198251293859 1.9649150371551514 0.4332832141387075 0.20022400268110596\n",
      "\n",
      "\n",
      "3500 tensor(3.7604e-07, device='cuda:0', grad_fn=<DivBackward0>) 0.0 2.406650128250476e-05\n",
      "Gan: tensor(-0.2935, device='cuda:0', grad_fn=<MaxBackward1>) tensor([1.2839e-09], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "2.414296691166306 1.9870845079421997 0.4272121832241065 0.1690185608159167\n",
      "\n",
      "\n",
      "3600 tensor(1.0670e-08, device='cuda:0', grad_fn=<DivBackward0>) 0.0 6.82868858348229e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-df34ff4bdbf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mopt_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mG_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                 \u001b[0mopt_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(int(echo)):\n",
    "\n",
    "    temp_number = 0\n",
    "    total_batch_loss = 0 \n",
    "        \n",
    "    loss2_list = []\n",
    "    loss1_sum = 0\n",
    "    loss2_sum = 0\n",
    "    denominator = 0\n",
    "    for index in range(0, BATCH_SIZE):\n",
    "        training_data_i, training_label, training_data = appen_train(\n",
    "            Generate_distribution(Agent_number_n))\n",
    "        h_loss1, h_loss2 = DiscriminatorNet(training_data_i, training_label,\n",
    "                                       training_data)\n",
    "        denominator += 1\n",
    "        loss1_sum += h_loss1\n",
    "        loss2_sum += h_loss2\n",
    "\n",
    "    loss_sum = torch.sum(loss1_sum + loss2_sum)\n",
    "    loss = (loss_sum) / denominator \n",
    "    total_batch_loss +=float(loss_sum)\n",
    "\n",
    "    opt_D.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_D.step()\n",
    "\n",
    "    temp_number = iteration\n",
    "    index_train_list.append(iteration)\n",
    "    train_losses.append(total_batch_loss)\n",
    "\n",
    "    if (iteration%100 == 0):\n",
    "        print(temp_number,loss,float(loss1_sum),float(loss2_sum))\n",
    "    ## Gan \n",
    "        if(Is_GAN):## Gan Work traning GeneratorNet\n",
    "\n",
    "            DiscriminatorNet.requires_grad = True\n",
    "            GeneratorNet.requires_grad = True\n",
    "\n",
    "\n",
    "            for step in range(100):\n",
    "                # real painting from artist\n",
    "                G_ideas = torch.randn(BATCH_SIZE, N_IDEAS,\n",
    "                                      requires_grad=True).to(dev)  # random ideas\\n\n",
    "                # fake painting from G (random ideas)\n",
    "\n",
    "                G_values = GeneratorNet(G_ideas)\n",
    "                G_values , indices = torch.sort(G_values, descending=True)\n",
    "            #     print(artist_paintings)\n",
    "            #     print(G_paintings)\n",
    "\n",
    "                result_list = []\n",
    "                for index in range(BATCH_SIZE):\n",
    "                    h_list = []\n",
    "                    value_list_tensor, S_tensor = redistribution_value_function(\n",
    "                        G_values[index])\n",
    "                    for i in range(Agent_number_n):\n",
    "                        h = DiscriminatorNet.calculate(\n",
    "                            value_list_tensor[i].cuda().type(torch.float32))\n",
    "                        h_list.append(h)\n",
    "                    h_list = torch.stack(h_list)\n",
    "                    result_list.append(torch.sum(h_list)/S_tensor.cuda())\n",
    "                result_list = torch.stack(result_list)\n",
    "\n",
    "                diff_loss = torch.max(result_list)-torch.min(result_list)\n",
    "                G_loss = torch.max(- diff_loss)\n",
    "\n",
    "                opt_G.zero_grad()\n",
    "                G_loss.backward()\n",
    "                opt_G.step()\n",
    "\n",
    "            # real painting from artist\n",
    "                G_ideas = torch.randn(BATCH_SIZE, N_IDEAS,\n",
    "                                      requires_grad=True).to(dev)  # random ideas\\n\n",
    "                # fake painting from G (random ideas)\n",
    "\n",
    "                G_values = GeneratorNet(G_ideas)\n",
    "                G_values , indices = torch.sort(G_values, descending=True)\n",
    "            #     print(artist_paintings)\n",
    "            #     print(G_paintings)\n",
    "\n",
    "                result_list = []\n",
    "                for index in range(BATCH_SIZE):\n",
    "                    h_list = []\n",
    "                    value_list_tensor, S_tensor = redistribution_value_function(\n",
    "                        G_values[index])\n",
    "                    for i in range(Agent_number_n):\n",
    "                        h = DiscriminatorNet.calculate(\n",
    "                            value_list_tensor[i].cuda().type(torch.float32))\n",
    "                        h_list.append(h)\n",
    "                    h_list = torch.stack(h_list)\n",
    "                    result_list.append(torch.sum(h_list)/S_tensor.cuda())\n",
    "                result_list = torch.stack(result_list)\n",
    "\n",
    "                diff_loss = torch.max(result_list)-torch.min(result_list)\n",
    "\n",
    "                D_loss = torch.where((Agent_number_n-1)>torch.min(result_list),\n",
    "                    torch.square(((Agent_number_n-1)-torch.min(result_list))),\n",
    "                    torch.zeros(1).to(dev)\n",
    "                  )   + torch.where((Agent_number_n-Alpha)<torch.max(result_list),\n",
    "                                torch.square((torch.max(result_list)-(Agent_number_n-Alpha)))/10000,\n",
    "                                torch.zeros(1).to(dev)\n",
    "                              )\n",
    "\n",
    "\n",
    "                opt_D.zero_grad()\n",
    "                D_loss.backward()\n",
    "                opt_D.step()\n",
    "                \n",
    "            print(\"Gan:\",G_loss,D_loss)\n",
    "            print()\n",
    "\n",
    "            \n",
    "        result_list = []\n",
    "        for index in range(len(testing_data)):\n",
    "            h_list = []\n",
    "            for i in range(Agent_number_n):\n",
    "                h = DiscriminatorNet.calculate(\n",
    "                    torch.tensor(testing_data[index][i]).to(dev).type(\n",
    "                        torch.float32))\n",
    "                h_list.append(float(h))\n",
    "            result_list.append(sum(h_list) / testing_label[index])\n",
    "        print(max(result_list), min(result_list),\n",
    "              max(result_list) - min(result_list),\n",
    "              (sum(result_list) / len(result_list) - min(result_list))+ Agent_number_n-1 - min(min(result_list),Agent_number_n-1) )\n",
    "\n",
    "        index_test_list.append(iteration)\n",
    "        test_losses.append(\n",
    "            (sum(result_list) / len(result_list) - min(result_list))+ Agent_number_n-1 - min(min(result_list),Agent_number_n-1) )\n",
    "        print()\n",
    "        index_test_list.append(iteration)\n",
    "        test_losses.append(\n",
    "            max(result_list)-min(result_list) )\n",
    "        \n",
    "  \n",
    "        print()\n",
    "\n",
    "\n",
    "    scheduler_D.step()\n",
    "    scheduler_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.441458Z",
     "start_time": "2021-06-12T15:59:22.980Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylim(0, 1.0)\n",
    "plt.plot(index_test_list,test_losses)\n",
    "plt.ylabel('test loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:12.932577Z",
     "start_time": "2021-06-12T17:15:12.914566Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(DiscriminatorNet, \"save/Deep_learning_D_3_1\")\n",
    "if(Is_GAN):\n",
    "    torch.save(GeneratorNet, \"save/Deep_learning_G_3_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.443452Z",
     "start_time": "2021-06-12T15:59:22.984Z"
    }
   },
   "outputs": [],
   "source": [
    "denominator = 0\n",
    "result_list = []\n",
    "for index in range(len(testing_data)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        h = DiscriminatorNet.calculate(torch.tensor(testing_data[index][i]).to(dev).type(torch.float32))\n",
    "        h_list.append(float(h))\n",
    "    \n",
    "    #print(sum(h_list),h_list,sum(h_list)/testing_label[index])\n",
    "    result_list.append(sum(h_list)/testing_label[index])\n",
    "    \n",
    "\n",
    "print(max(result_list),min(result_list),max(result_list)-min(result_list))\n",
    "print(sum(result_list)/len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.444449Z",
     "start_time": "2021-06-12T15:59:22.986Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list)/len(result_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.445446Z",
     "start_time": "2021-06-12T15:59:22.988Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate 10000 testing data on GeneratorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.447442Z",
     "start_time": "2021-06-12T15:59:22.990Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "GeneratorNet = torch.load(\"save/Deep_learning_G_3_1\")\n",
    "def appen_test_G(x_list):\n",
    "    global temp_list\n",
    "                \n",
    "    S= max(sum(x_list),1.0);\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in range(Agent_number_n):\n",
    "        temp = []\n",
    "        for j in range(Agent_number_n):\n",
    "            if(i!=j):\n",
    "                temp.append(x_list[j])\n",
    "        appen(temp,S)\n",
    "    testing_data_G.append(temp_list)\n",
    "    testing_label_G.append(S)\n",
    "# fake painting from G (random ideas)\n",
    "\n",
    "def read_testing_data_G():\n",
    "    for i in range(10000):\n",
    "        #appen_test_G(sorted(np.random.rand(Agent_number_n), reverse=True));\n",
    "        G_ideas = torch.randn(N_IDEAS).to(dev)  # random ideas\\n\n",
    "        G_values = GeneratorNet(G_ideas)\n",
    "        G_values , indices = torch.sort(G_values, descending=True)\n",
    "        appen_test_G(G_values.detach().cpu().numpy()) \n",
    "        \n",
    "testing_data_G = []\n",
    "testing_label_G = []\n",
    "read_testing_data_G()\n",
    "testing_data_G=np.array(testing_data_G)\n",
    "testing_label_G=np.array(testing_label_G)\n",
    "print(testing_data_G)\n",
    "print(testing_label_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.448439Z",
     "start_time": "2021-06-12T15:59:22.993Z"
    }
   },
   "outputs": [],
   "source": [
    "result_list_G = []\n",
    "for index in range(len(testing_data_G)):\n",
    "    h_list = []\n",
    "    for i in range(Agent_number_n):\n",
    "        h = DiscriminatorNet.calculate(torch.tensor(testing_data_G[index][i]).to(dev).type(torch.float32))\n",
    "        h_list.append(float(h))\n",
    "    \n",
    "    #print(sum(h_list),h_list,sum(h_list)/testing_label[index])\n",
    "    result_list_G.append(sum(h_list)/testing_label_G[index])\n",
    "    \n",
    "\n",
    "print(max(result_list_G),min(result_list_G),max(result_list_G)-min(result_list_G))\n",
    "print(sum(result_list_G)/len(result_list_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.449437Z",
     "start_time": "2021-06-12T15:59:22.995Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(result_list_G,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(result_list_G)/len(result_list_G), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "result_list_G.sort()\n",
    "\n",
    "plt.axvline(x=result_list[int(len(result_list_G)*0.01)], linestyle='--', linewidth=0.5, label=\"1%\", c='b')\n",
    "plt.axvline(x=result_list[int(len(result_list_G)*0.99)], linestyle='--', linewidth=0.5, label=\"99%\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T17:15:08.450434Z",
     "start_time": "2021-06-12T15:59:22.996Z"
    }
   },
   "outputs": [],
   "source": [
    "final_list = result_list + result_list_G\n",
    "\n",
    "\n",
    "plt.hist(final_list,bins=500)\n",
    "\n",
    "plt.title(\"Means\", y=1.015, fontsize=20)\n",
    "plt.axvline(x=sum(final_list)/len(final_list), linestyle='--', linewidth=2.5, label=\"sample mean\", c='orange')\n",
    "plt.xlabel(\"samples\", labelpad=14)\n",
    "plt.ylabel(\"frequency of occurence\", labelpad=14)\n",
    "plt.legend();\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
